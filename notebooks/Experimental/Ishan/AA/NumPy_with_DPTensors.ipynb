{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c74cc9-fb3f-4872-8e62-ef181c4a3011",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Implement LeakyRelu activation function --> Done\n",
    "    - Accidentally added it to Conv layer; we need it on the BatchNorm layer for our model\n",
    "- Implement forward pass & backward pass for every layer of our CNN in pure NumPy: --> Done\n",
    "    - conv2d --> Done\n",
    "    - batchnorm2d  --> Done\n",
    "    - avgpool2d  --> Done\n",
    "    - maxpool2d  --> Done\n",
    "    - linear  --> Done\n",
    "- Implement AdaMax optimizer in pure NumPy --> Done\n",
    "- Implement Cross-Entropy Loss in pure NumPy --> Done\n",
    "- Implement Model class in pure NumPy --> Done\n",
    "- Verify pure NumPy model training works --> Done\n",
    "- Modify to work with DP Tensors instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c771e5-8f3b-4158-8dad-b40a55c49f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda3/envs/syft/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from syft import PhiTensor\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9570ecfe-9aeb-4c4d-9f4c-b5e250993cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_leakyrelu(dp_tensor: PhiTensor, slope: float=0.01) -> PhiTensor:\n",
    "    # TODO: Should we have an index in DSLs that corresponds to no data?\n",
    "    \n",
    "    gt = (dp_tensor.child > 0)\n",
    "    return PhiTensor(\n",
    "        child= gt * dp_tensor.child + (1 - gt) * dp_tensor.child * slope,\n",
    "        data_subjects=dp_tensor.data_subjects,\n",
    "        min_vals= lra(data=dp_tensor.min_vals.data * slope, shape=dp_tensor.min_vals.shape), \n",
    "        max_vals= lra(data=dp_tensor.max_vals.data * slope, shape=dp_tensor.max_vals.shape),\n",
    "    )\n",
    "\n",
    "\n",
    "class leaky_ReLU():\n",
    "\n",
    "    def __init__(self, slope=0.01):\n",
    "        super(leaky_ReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def forward(self, input_array: PhiTensor):\n",
    "        # Last image that has been forward passed through this activation function\n",
    "        self.last_forward = input_array        \n",
    "        return dp_leakyrelu(dp_tensor=input_array, slope=self.slope)\n",
    "\n",
    "    def derivative(self, input_array: Optional[PhiTensor]=None):\n",
    "        last_forward = input_array if input_array else self.last_forward\n",
    "        res = np.ones(last_forward.shape)\n",
    "        idx = last_forward <= 0\n",
    "        res[idx.child] = self.slope\n",
    "        \n",
    "        return PhiTensor(child=res,\n",
    "                         data_subjects=last_forward.data_subjects,\n",
    "                         min_vals=last_forward.min_vals*0,\n",
    "                         max_vals = last_forward.max_vals*1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fd353b-f0fe-4423-b919-9961f86e46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uniform():\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "\n",
    "    def call(self, size):\n",
    "        return np.array(np.random.uniform(-self.scale, self.scale, size=size))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcc3c8d-5f35-41ae-9cfd-dee729717732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_size(size):\n",
    "    if len(size) == 2:\n",
    "        fan_in = size[0]\n",
    "        fan_out = size[1]\n",
    "\n",
    "    elif len(size) == 4 or len(size) == 5:\n",
    "        respective_field_size = np.prod(size[2:])\n",
    "        fan_in = size[1] * respective_field_size\n",
    "        fan_out = size[0] * respective_field_size\n",
    "\n",
    "    else:\n",
    "        fan_in = fan_out = int(np.sqrt(np.prod(size)))\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f67671-2b55-43d4-91a2-7165e50601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitialization():\n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "    \n",
    "    def call(self, size):\n",
    "        fan_in, fan_out = decompose_size(size)\n",
    "        return Uniform(np.sqrt(6 / (fan_in + fan_out)))(size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a656e1d-8003-4529-a985-ff7885cd553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Subclassed when implementing new types of layers.\n",
    "    \n",
    "    Each layer can keep track of the layer(s) feeding into it, a\n",
    "    network's output :class:`Layer` instance can double as a handle to the full\n",
    "    network.\n",
    "    \"\"\"\n",
    "\n",
    "    first_layer = False\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \n",
    "        \n",
    "        Returns a list of numpy.array variables or expressions that\n",
    "        parameterize the layer.\n",
    "        Returns\n",
    "        -------\n",
    "        list of numpy.array variables or expressions\n",
    "            A list of variables that parameterize the layer\n",
    "        Notes\n",
    "        -----\n",
    "        For layers without any parameters, this will return an empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from backward(). \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def param_grads(self):\n",
    "        \"\"\" Layer parameters and corresponding gradients. \"\"\"\n",
    "        return list(zip(self.params, self.grads))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fb14f-9793-49f5-aaa8-498c43e86e71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "In the Convolution layer below, W, dw, b, db all start as numpy arrays.\n",
    "They **should** be PhiTensors- if someone wants to print them, they should have to spend PB.\n",
    "\n",
    "However, when they're initialized in the Convolution layer, they have to be initialized as \n",
    "np arrays because the initial values are public information. But as soon as they are exposed to a DP Tensor, they should convert to a PhiTensor.\n",
    "\n",
    "However we should be able to pass the data to and from various layers to each other.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf3cc8-0c49-452f-9c05-ebffded0b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \"\"\"\n",
    "    If this is the first layer in a model, provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does NOT include the sample axis, N.),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_filter, filter_size, input_shape=None, stride=1):\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.out_shape = None\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.init = XavierInitialization()\n",
    "        self.activation = leaky_ReLU()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.input_shape is not None\n",
    "            input_shape = self.input_shape\n",
    "        else:\n",
    "            input_shape = prev_layer.out_shape\n",
    "\n",
    "        # input_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert len(input_shape) == 4\n",
    "\n",
    "        nb_batch, pre_nb_filter, pre_height, pre_width = input_shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        height = (pre_height - filter_height) // self.stride + 1\n",
    "        width = (pre_width - filter_width) // self.stride + 1\n",
    "\n",
    "        # output shape\n",
    "        self.out_shape = (nb_batch, self.nb_filter, height, width)\n",
    "        print(self.out_shape)\n",
    "\n",
    "        # filters\n",
    "        self.W = self.init((self.nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "        self.b = np.zeros((self.nb_filter,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # TODO: This could fail if the DP Tensor has < 4 dimensions\n",
    "        \n",
    "        # shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = input.shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "        # init\n",
    "        outputs = np.zeros((nb_batch, self.nb_filter, new_img_h, new_img_w))\n",
    "        \n",
    "        \n",
    "\n",
    "        # convolution operation\n",
    "        for x in np.arange(nb_batch):\n",
    "            for y in np.arange(self.nb_filter):\n",
    "                for h in np.arange(new_img_h):\n",
    "                    for w in np.arange(new_img_w):\n",
    "                        h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                        # patch: (input_depth, filter_h, filter_w)\n",
    "                        patch = input[x, :, h_shift: h_shift + filter_height, w_shift: w_shift + filter_width]\n",
    "                        outputs[x, y, h, w] = np.sum(patch.child * self.W[y]) + self.b[y]\n",
    "\n",
    "        # nonlinear activation\n",
    "        # self.last_output: (nb_batch, output_depth, image height, image width)\n",
    "        \n",
    "        # TODO: Min/max vals are direct function of private data- fix this when we have time\n",
    "        outputs = PhiTensor(\n",
    "            child=outputs,data_subjects=np.zeros_like(outputs), \n",
    "            min_vals=outputs.min(), max_vals=outputs.max()\n",
    "        )\n",
    "        self.last_output = self.activation.forward(outputs)\n",
    "\n",
    "        return self.last_output\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        assert pre_grad.shape == self.last_output.shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = self.last_input.shape\n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "        \n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "#         filter_h, filter_w = self.filter_size\n",
    "        old_img_h, old_img_w = self.last_input.shape[-2:]\n",
    "\n",
    "        # gradients\n",
    "        self.dW = np.zeros((self.W.shape))\n",
    "        self.db = np.zeros((self.b.shape))\n",
    "        delta = pre_grad * self.activation.derivative()\n",
    "\n",
    "        # dW\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            for t in np.arange(input_depth):\n",
    "                for h in np.arange(filter_height):\n",
    "                    for w in np.arange(filter_width):\n",
    "                        input_window = self.last_input[:, t,\n",
    "                                       h:old_img_h - filter_height + h + 1:self.stride,\n",
    "                                       w:old_img_w - filter_width + w + 1:self.stride]\n",
    "                        delta_window = delta[:, r]\n",
    "                        self.dW[r, t, h, w] = ((input_window * delta_window).sum() * (1/nb_batch)).child\n",
    "        # db\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            self.db[r] = (delta[:, r].sum() * (1/nb_batch)).child\n",
    "        \n",
    "        \n",
    "        # dX\n",
    "        \n",
    "        \n",
    "        if not self.first_layer:\n",
    "            layer_grads = self.last_input.zeros_like()\n",
    "            for b in np.arange(nb_batch):\n",
    "                for r in np.arange(self.nb_filter):\n",
    "                    for t in np.arange(input_depth):\n",
    "                        for h in np.arange(new_img_h):\n",
    "                            for w in np.arange(new_img_w):\n",
    "                                h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                                temp = layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width]\n",
    "                                layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width] = temp+ (delta[b, r, h, w] * self.W[r, t])\n",
    "                              \n",
    "        return layer_grads\n",
    "                         \n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bfa725-40be-4c48-9cd8-0c583a34c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Convolution(3, 3, input_shape=(1, 3, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6480805b-c623-4f5a-83b8-7bdc0dd3b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "c.connect_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454d6256-d500-4669-a8f4-61d96c4d4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhiTensor(\n",
    "    child=np.random.rand(1, 3, 10, 10), \n",
    "    data_subjects=np.zeros((1,3,10,10)), min_vals=0, max_vals=1)\n",
    "\n",
    "res = c.forward(input=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a24b8863-393e-4346-ad8c-c7e30a7fb045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 8, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.last_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06a0337-2b43-42d1-901c-07c6b81d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b_data = PhiTensor(\n",
    "    child=np.random.rand(1,3,8,8), \n",
    "    data_subjects=np.zeros((1,3,8,8)), min_vals=0, max_vals=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa44336-f164-4ce5-a101-7eb26921fb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[-0.01368677  0.07655184  0.26808735  0.26139304  0.2892576\n",
       "     0.36479545  0.18882884  0.19502596  0.33179105  0.18431865]\n",
       "   [ 0.02786342  0.12780637  0.41951382  0.44707183  0.39988462\n",
       "     0.62914763  0.52097702  0.55511052  0.62023862  0.44333853]\n",
       "   [-0.01051321 -0.01506715  0.31740432  0.38767782 -0.00118546\n",
       "     0.18399839  0.22044539  0.23608878  0.39467043  0.39958055]\n",
       "   [ 0.04440349  0.12096326  0.44062383  0.35904946 -0.07292287\n",
       "     0.39873504 -0.34745802 -0.00618057  0.35158452 -0.03718686]\n",
       "   [ 0.01797175 -0.21735303 -0.07056014  0.06990411  0.3604693\n",
       "     0.33610087  0.22053634 -0.20686697  0.04416212  0.06957678]\n",
       "   [ 0.01457106 -0.12682011  0.13566842  0.38062968  0.35595641\n",
       "     0.33340349  0.53418683  0.37895081  0.85590994  0.66593157]\n",
       "   [-0.0141973   0.03925299  0.34655281  0.13295765  0.3676245\n",
       "     0.23561997  0.28310951  0.37813913  0.3592548   0.37937526]\n",
       "   [-0.13982254  0.2300639   0.18410419 -0.12943956  0.18523199\n",
       "     0.07548979  0.0969464  -0.06642111  0.10146347  0.2626171 ]\n",
       "   [-0.0148135  -0.32032542 -0.18452363 -0.11365298  0.18886421\n",
       "     0.19296506  0.2559101  -0.14750347 -0.02459659  0.15924553]\n",
       "   [ 0.00447599  0.0494415   0.16672678 -0.1359996  -0.16476399\n",
       "    -0.28217162 -0.08771595 -0.08543062 -0.19469901 -0.01214126]]\n",
       "\n",
       "  [[-0.04315183  0.02044886 -0.0373964  -0.10004305  0.00998539\n",
       "    -0.09019946 -0.09274964  0.00632899 -0.03231025 -0.08292879]\n",
       "   [ 0.17391788  0.16974996 -0.00504781  0.02619862  0.0029676\n",
       "    -0.25827275  0.09824298 -0.04595898 -0.21227713 -0.25673213]\n",
       "   [ 0.02521606  0.33965945  0.09138296  0.24899176  0.29778616\n",
       "     0.31269327  0.03812404  0.32124965 -0.06194088 -0.23202048]\n",
       "   [ 0.28034045  0.36302192  0.02831403 -0.28346881  0.32121691\n",
       "     0.03943593  0.38617206  0.52502455 -0.32222462 -0.27624228]\n",
       "   [ 0.1448115   0.34531017  0.10299599  0.53769305  0.14784337\n",
       "    -0.35515509 -0.03609961 -0.00992247 -0.39245522 -0.32599137]\n",
       "   [ 0.09975182  0.1482245   0.28328122  0.02429891 -0.14068161\n",
       "     0.16652351 -0.26518252  0.10671667 -0.13686311 -0.40731854]\n",
       "   [-0.0274738   0.27850072  0.21722951  0.26667223  0.09122545\n",
       "     0.16086867  0.04178184  0.23040068 -0.08824542 -0.22136673]\n",
       "   [ 0.37586194  0.13592485 -0.20378302  0.17128783  0.05903719\n",
       "     0.20634324  0.37428276  0.25911044  0.12524373 -0.11441589]\n",
       "   [ 0.16163667  0.05167799  0.02812582  0.20551637  0.15459284\n",
       "    -0.1229791  -0.10586138  0.34435277  0.04821661 -0.11039294]\n",
       "   [-0.17690949  0.04431924 -0.04912167  0.08945492  0.18899176\n",
       "     0.26037845  0.10690296  0.07891619  0.17292519  0.03142846]]\n",
       "\n",
       "  [[ 0.14860128  0.25662415  0.29906955  0.39675453  0.3377471\n",
       "     0.27185279  0.30173692  0.30934935  0.18253038  0.0713066 ]\n",
       "   [ 0.17312291  0.09091773  0.06286198  0.310759    0.09767846\n",
       "     0.37294882  0.44852646  0.32031464  0.08230321  0.15372981]\n",
       "   [ 0.41222163  0.43521464  0.40896004  0.57963287  0.07568371\n",
       "     0.21572961  0.30738705  0.34866299 -0.01757721  0.21330551]\n",
       "   [ 0.35035184  0.06836159 -0.01491875  0.34561523  0.5728289\n",
       "     0.4807209   0.782772    0.53442276  0.17916551  0.21905239]\n",
       "   [ 0.36291676  0.04848764  0.37244688  0.47634373  0.36125336\n",
       "    -0.08010422  0.88251483  0.12461574  0.30563537  0.37289842]\n",
       "   [ 0.18027792  0.02929345  0.44341759  0.3459179   0.25170318\n",
       "     0.3846041   0.45823603  0.18366241  0.11360032  0.15389112]\n",
       "   [ 0.28287736  0.26231194  0.49329261 -0.0080785   0.25132554\n",
       "     0.06325915  0.2584379   0.35567911 -0.13310823  0.22374798]\n",
       "   [ 0.408187    0.13190479  0.40689354  0.26125626  0.57708043\n",
       "     0.38940891  0.45168726  0.22862397  0.03008307  0.22360299]\n",
       "   [ 0.38813295 -0.14382456  0.36427132  0.0490913   0.07058163\n",
       "    -0.13669464  0.21344642  0.0233925  -0.11333413  0.09850935]\n",
       "   [-0.08868047 -0.10110592  0.13317121 -0.0010609   0.17885\n",
       "    -0.01425697  0.13310982  0.10904518 -0.05090333  0.07284854]]]], min_vals=<lazyrepeatarray data: -0.6746418542753151 -> shape: (1, 3, 10, 10)>, max_vals=<lazyrepeatarray data: 0.8825148324561666 -> shape: (1, 3, 10, 10)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.backward(b_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df10cfa0-63cd-431b-bccd-d5c6b72516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, epsilon=1e-6, momentum=0.9, axis=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "\n",
    "        self.beta, self.dbeta = None, None\n",
    "        self.gamma, self.dgamma = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        n_in = prev_layer.out_shape[-1]\n",
    "        self.beta = np.zeros((n_in,))\n",
    "        self.gamma = np.ones((n_in,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        # N, D = x.shape\n",
    "        self.out_shape = input.shape\n",
    "\n",
    "        # step1: calculate the mean\n",
    "        xmu = input - input.mean(axis=0)\n",
    "        print(\"Subtracted mean\")\n",
    "        # step3:        \n",
    "        var = xmu.std(axis=0)\n",
    "        sqrtvar = (var + self.epsilon).sqrt()\n",
    "    \n",
    "        print(\"square rooted\")\n",
    "        ivar = sqrtvar.reciprocal()\n",
    "#         ivar = PhiTensor(\n",
    "#             child=sqrtvar.child ** -1,\n",
    "#             data_subjects=sqrtvar.data_subjects,\n",
    "#             min_vals=lra(data=1/sqrtvar.min_vals.data,shape=sqrtvar.shape),\n",
    "#             max_vals=lra(data=1/sqrtvar.max_vals.data,shape=sqrtvar.shape)\n",
    "#         )\n",
    "\n",
    "        \n",
    "        print(\"got past reciprocal\")\n",
    "\n",
    "\n",
    "        # step5: normalization->x^\n",
    "        xhat = xmu * ivar\n",
    "        print(\"got xhat\")\n",
    "\n",
    "        # step6: scale and shift\n",
    "        gammax = xhat * self.gamma\n",
    "        print(\"got gammax\")\n",
    "        out = gammax + self.beta\n",
    "        print(\"got out\")\n",
    "\n",
    "        self.cache = (xhat, xmu, ivar, sqrtvar, var)\n",
    "        print(\"cached\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        xhat, xmu, ivar, sqrtvar, var = self.cache\n",
    "\n",
    "        N, D = pre_grad.shape\n",
    "\n",
    "        # step6\n",
    "        self.dbeta = np.sum(pre_grad, axis=0)\n",
    "        dgammax = pre_grad\n",
    "        self.dgamma = np.sum(dgammax * xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "\n",
    "        # step5\n",
    "        divar = np.sum(dxhat * xmu, axis=0)\n",
    "        dxmu1 = dxhat * ivar \n",
    "\n",
    "        # step4\n",
    "        dsqrtvar = -1. / (sqrtvar ** 2) * divar\n",
    "        dvar = 0.5 * 1. / np.sqrt(var + self.epsilon) * dsqrtvar\n",
    "\n",
    "        # step3\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        dxmu2 = 2 * xmu * dsq  \n",
    "\n",
    "        # step2, \n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "\n",
    "        # step1, \n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "\n",
    "        # step0 done!\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self.gamma\n",
    "\n",
    "    @property\n",
    "    def grades(self):\n",
    "        return self.dbeta, self.dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e178933-c93b-4580-880f-22709d5527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bnc():\n",
    "    shape = (10, 3, 50, 50)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    return bn.forward(c_out), bn\n",
    "    \n",
    "# test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfdd83a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 48, 48)\n",
      "Subtracted mean\n",
      "square rooted\n",
      "got past reciprocal\n",
      "got xhat\n",
      "got gammax\n",
      "got out\n",
      "cached\n"
     ]
    }
   ],
   "source": [
    "output, bn = test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16e99e-20fa-4702-bf4a-afed82b43525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(Layer):\n",
    "    \"\"\"Average pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.out_shape = 0\n",
    "        self.out_shape = None\n",
    "        self.input_shape = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert 5 > len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].mean()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = np.mean(input[a, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        length = np.prod(self.pool_size)\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                            layer_grads[a, b, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                                pre_grad[a, b, h, w] / length\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                        layer_grads[a, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                            pre_grad[a, h, w] / length\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads\n",
    "\n",
    "\n",
    "class MaxPool(Layer):\n",
    "    \"\"\"Max pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.out_shape = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        # prev_layer.out_shape: (nb_batch, ..., height, width)\n",
    "        assert len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        self.last_input = input\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = input[a, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            patch = self.last_input[a, b, h:h + pool_h, w:w + pool_w]\n",
    "                            max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                            h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                            layer_grads[a, b, h_shift, w_shift] = pre_grad[a, b, a, w]\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        patch = self.last_input[a, h:h + pool_h, w:w + pool_w]\n",
    "                        max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                        h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                        layer_grads[a, h_shift, w_shift] = pre_grad[a, a, w]\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508b16d-6fa3-425d-9898-84171cd41258",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = AvgPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5506a6-4225-4d5d-b121-406d41b930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_avg():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    avg = AvgPool((2,2))\n",
    "    avg.connect_to(bn)\n",
    "    return avg.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b8c41-ef8b-421a-bc4e-52ea7f7ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp = MaxPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f035d2e-ecae-4fad-985e-4c9fa6a683bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_max():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    maxp = MaxPool((2,2))\n",
    "    maxp.connect_to(bn)\n",
    "    return maxp.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41a5cc-2098-438e-9e7c-d9978fadcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.out_shape = (None, n_out)\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.last_input = None\n",
    "        self.init = XavierInitialization()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.n_in is not None\n",
    "            n_in = self.n_in\n",
    "        else:\n",
    "            assert len(prev_layer.out_shape) == 2\n",
    "            n_in = prev_layer.out_shape[-1]\n",
    "\n",
    "        self.W = self.init((n_in, self.n_out))\n",
    "        self.b = np.zeros((self.n_out,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        self.last_input = input\n",
    "        return input.dot(self.W) + self.b\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        self.dW = np.dot(self.last_input.T, pre_grad)\n",
    "        self.db = np.mean(pre_grad, axis=0)\n",
    "        if not self.first_layer:\n",
    "            return np.dot(pre_grad, self.W.T)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d766d-997b-4360-bd26-77ca4b512447",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(n_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e785a6-4581-464a-8cab-e411bbbe6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lin():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "    \n",
    "    lin = Linear(n_out=2)\n",
    "    lin.n_in = 6\n",
    "    lin.connect_to()\n",
    "    return lin.forward(smol_data)\n",
    "    \n",
    "test_lin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814059b-a806-4aa6-b8b9-6542bd98344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"Abstract optimizer base class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clip : float\n",
    "        If smaller than 0, do not apply parameter clip.\n",
    "    lr : float\n",
    "        The learning rate controlling the size of update steps\n",
    "    decay : float\n",
    "        Decay parameter for the moving average. Must lie in [0, 1) where\n",
    "        lower numbers means a shorter “memory”.\n",
    "    lr_min : float\n",
    "        When adapting step rates, do not move below this value. Default is 0.\n",
    "    lr_max : float\n",
    "        When adapting step rates, do not move above this value. Default is inf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.decay = decay\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iterations += 1\n",
    "\n",
    "        self.lr *= (1. / 1 + self.decay * self.iterations)\n",
    "        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17351b5-fd14-4dce-910a-a7591f7f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adamax(Optimizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta1 : float\n",
    "        Exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        Exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        Constant for numerical stability.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
    "           Adam: A Method for Stochastic Optimization.\n",
    "           arXiv preprint arXiv:1412.6980.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, *args, **kwargs):\n",
    "        super(Adamax, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.ms = None\n",
    "        self.vs = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        # init\n",
    "        self.iterations += 1\n",
    "        a_t = self.lr / (1 - np.power(self.beta1, self.iterations))\n",
    "        if self.ms is None:\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "        if self.vs is None:\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "\n",
    "        # update parameters\n",
    "        for i, (m, v, p, g) in enumerate(zip(self.ms, self.vs, params, grads)):\n",
    "            m = self.beta1 * m + (1 - self.beta1) * g\n",
    "            v = np.maximum(self.beta2 * v, np.abs(g))\n",
    "            p -= a_t * m / (v + self.epsilon)\n",
    "\n",
    "            self.ms[i] = m\n",
    "            self.vs[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735de6c7-43c7-49ec-aa2b-b40106a63452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "    def __init__(self, epsilon=1e-11):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        .. math:: L = -t \\\\log(p) - (1 - t) \\\\log(1 - p)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n",
    "        return np.mean(-np.sum(targets * np.log(outputs) + (1 - targets) * np.log(1 - outputs), axis=1))\n",
    "\n",
    "    def backward(self, outputs, targets):\n",
    "        \"\"\"Backward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n",
    "        divisor = np.maximum(outputs * (1 - outputs), self.epsilon)\n",
    "        return (outputs - targets) / divisor\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0d5b1-0240-4871-91d1-aa0797b1595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "\n",
    "        self.loss = None\n",
    "        self.optimizer = Adamax\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer), \"PySyft doesn't recognize this kind of layer.\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss=BinaryCrossEntropy(), optimizer=Adamax()):\n",
    "        self.layers[0].first_layer = True\n",
    "\n",
    "        next_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer.connect_to(next_layer)\n",
    "            next_layer = layer\n",
    "\n",
    "        self.loss = BinaryCrossEntropy()\n",
    "        self.optimizer = Adamax()\n",
    "\n",
    "    def fit(self, X, Y, max_iter=100, batch_size=64, shuffle=True,\n",
    "            validation_split=0., validation_data=None):\n",
    "\n",
    "        # prepare data\n",
    "        train_X = X #.astype(get_dtype()) if np.issubdtype(np.float64, X.dtype) else X\n",
    "        train_Y = Y #.astype(get_dtype()) if np.issubdtype(np.float64, Y.dtype) else Y\n",
    "\n",
    "        if 1. > validation_split > 0.:\n",
    "            split = int(train_Y.shape[0] * validation_split)\n",
    "            valid_X, valid_Y = train_X[-split:], train_Y[-split:]\n",
    "            train_X, train_Y = train_X[:-split], train_Y[:-split]\n",
    "        elif validation_data is not None:\n",
    "            valid_X, valid_Y = validation_data\n",
    "        else:\n",
    "            valid_X, valid_Y = None, None\n",
    "\n",
    "        iter_idx = 0\n",
    "        while iter_idx < max_iter:\n",
    "            iter_idx += 1\n",
    "\n",
    "            # shuffle\n",
    "            if shuffle:\n",
    "                seed = np.random.randint(111, 1111111)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_X)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_Y)\n",
    "\n",
    "            # train\n",
    "            train_losses, train_predicts, train_targets = [], [], []\n",
    "            for b in range(train_Y.shape[0] // batch_size):\n",
    "                batch_begin = b * batch_size\n",
    "                batch_end = batch_begin + batch_size\n",
    "                x_batch = train_X[batch_begin:batch_end]\n",
    "                y_batch = train_Y[batch_begin:batch_end]\n",
    "\n",
    "                # forward propagation\n",
    "                y_pred = self.predict(x_batch)\n",
    "\n",
    "                # backward propagation\n",
    "                next_grad = self.loss.backward(y_pred, y_batch)\n",
    "                for layer in self.layers[::-1]:\n",
    "                    next_grad = layer.backward(next_grad)\n",
    "\n",
    "                # get parameter and gradients\n",
    "                params = []\n",
    "                grads = []\n",
    "                for layer in self.layers:\n",
    "                    params += layer.params\n",
    "                    grads += layer.grads\n",
    "\n",
    "                # update parameters\n",
    "                self.optimizer.update(params, grads)\n",
    "\n",
    "                # got loss and predict\n",
    "                train_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                train_predicts.extend(y_pred)\n",
    "                train_targets.extend(y_batch)\n",
    "\n",
    "            # output train status\n",
    "            runout = \"iter %d, train-[loss %.4f, acc %.4f]; \" % (\n",
    "                iter_idx, float(np.mean(train_losses)), float(self.accuracy(train_predicts, train_targets)))\n",
    "\n",
    "            # runout = \"iter %d, train-[loss %.4f, ]; \" % (\n",
    "            #     iter_idx, float(np.mean(train_losses)))\n",
    "\n",
    "            if valid_X is not None and valid_Y is not None:\n",
    "                # valid\n",
    "                valid_losses, valid_predicts, valid_targets = [], [], []\n",
    "                for b in range(valid_X.shape[0] // batch_size):\n",
    "                    batch_begin = b * batch_size\n",
    "                    batch_end = batch_begin + batch_size\n",
    "                    x_batch = valid_X[batch_begin:batch_end]\n",
    "                    y_batch = valid_Y[batch_begin:batch_end]\n",
    "\n",
    "                    # forward propagation\n",
    "                    y_pred = self.predict(x_batch)\n",
    "\n",
    "                    # got loss and predict\n",
    "                    valid_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                    valid_predicts.extend(y_pred)\n",
    "                    valid_targets.extend(y_batch)\n",
    "\n",
    "                # output valid status\n",
    "                runout += \"valid-[loss %.4f, acc %.4f]; \" % (\n",
    "                    float(np.mean(valid_losses)), float(self.accuracy(valid_predicts, valid_targets)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        x_next = X\n",
    "        for layer in self.layers[:]:\n",
    "            x_next = layer.forward(x_next)\n",
    "        y_pred = x_next\n",
    "        return y_pred\n",
    "\n",
    "    def accuracy(self, outputs, targets):\n",
    "        y_predicts = np.argmax(outputs, axis=1)\n",
    "        y_targets = np.argmax(targets, axis=1)\n",
    "        acc = y_predicts == y_targets\n",
    "        return np.mean(acc)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dce0b9-cd55-45f3-9603-03024945cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93cd4e-5503-4431-978b-c0074840a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
