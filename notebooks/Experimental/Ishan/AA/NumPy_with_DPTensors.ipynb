{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c74cc9-fb3f-4872-8e62-ef181c4a3011",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Implement LeakyRelu activation function --> Done\n",
    "    - Accidentally added it to Conv layer; we need it on the BatchNorm layer for our model\n",
    "- Implement forward pass & backward pass for every layer of our CNN in pure NumPy: --> Done\n",
    "    - conv2d --> Done\n",
    "    - batchnorm2d  --> Done\n",
    "    - avgpool2d  --> Done\n",
    "    - maxpool2d  --> Done\n",
    "    - linear  --> Done\n",
    "- Implement AdaMax optimizer in pure NumPy --> Done\n",
    "- Implement Cross-Entropy Loss in pure NumPy --> Done\n",
    "- Implement Model class in pure NumPy --> Done\n",
    "- Verify pure NumPy model training works --> Done\n",
    "- Modify to work with DP Tensors instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c771e5-8f3b-4158-8dad-b40a55c49f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Hagrid/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft import PhiTensor\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9570ecfe-9aeb-4c4d-9f4c-b5e250993cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_leakyrelu(dp_tensor: PhiTensor, slope: float=0.01) -> PhiTensor:\n",
    "    # TODO: Should we have an index in DSLs that corresponds to no data?\n",
    "    \n",
    "    gt = (dp_tensor.child > 0)\n",
    "    return PhiTensor(\n",
    "        child= gt * dp_tensor.child + (1 - gt) * dp_tensor.child * slope,\n",
    "        data_subjects=dp_tensor.data_subjects,\n",
    "        min_vals= lra(data=dp_tensor.min_vals.data * slope, shape=dp_tensor.min_vals.shape), \n",
    "        max_vals= lra(data=dp_tensor.max_vals.data * slope, shape=dp_tensor.max_vals.shape),\n",
    "    )\n",
    "\n",
    "\n",
    "class leaky_ReLU():\n",
    "\n",
    "    def __init__(self, slope=0.01):\n",
    "        super(leaky_ReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def forward(self, input_array: PhiTensor):\n",
    "        # Last image that has been forward passed through this activation function\n",
    "        self.last_forward = input_array        \n",
    "        return dp_leakyrelu(dp_tensor=input_array, slope=self.slope)\n",
    "\n",
    "    def derivative(self, input_array: Optional[PhiTensor]=None):\n",
    "        last_forward = input_array if input_array else self.last_forward\n",
    "        res = np.ones(last_forward.shape)\n",
    "        res[last_forward <= 0] = self.slope\n",
    "        return res\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fd353b-f0fe-4423-b919-9961f86e46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uniform():\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "\n",
    "    def call(self, size):\n",
    "        return np.array(np.random.uniform(-self.scale, self.scale, size=size))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcc3c8d-5f35-41ae-9cfd-dee729717732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_size(size):\n",
    "    if len(size) == 2:\n",
    "        fan_in = size[0]\n",
    "        fan_out = size[1]\n",
    "\n",
    "    elif len(size) == 4 or len(size) == 5:\n",
    "        respective_field_size = np.prod(size[2:])\n",
    "        fan_in = size[1] * respective_field_size\n",
    "        fan_out = size[0] * respective_field_size\n",
    "\n",
    "    else:\n",
    "        fan_in = fan_out = int(np.sqrt(np.prod(size)))\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f67671-2b55-43d4-91a2-7165e50601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitialization():\n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "    \n",
    "    def call(self, size):\n",
    "        fan_in, fan_out = decompose_size(size)\n",
    "        return Uniform(np.sqrt(6 / (fan_in + fan_out)))(size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a656e1d-8003-4529-a985-ff7885cd553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Subclassed when implementing new types of layers.\n",
    "    \n",
    "    Each layer can keep track of the layer(s) feeding into it, a\n",
    "    network's output :class:`Layer` instance can double as a handle to the full\n",
    "    network.\n",
    "    \"\"\"\n",
    "\n",
    "    first_layer = False\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \n",
    "        \n",
    "        Returns a list of numpy.array variables or expressions that\n",
    "        parameterize the layer.\n",
    "        Returns\n",
    "        -------\n",
    "        list of numpy.array variables or expressions\n",
    "            A list of variables that parameterize the layer\n",
    "        Notes\n",
    "        -----\n",
    "        For layers without any parameters, this will return an empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from backward(). \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def param_grads(self):\n",
    "        \"\"\" Layer parameters and corresponding gradients. \"\"\"\n",
    "        return list(zip(self.params, self.grads))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fb14f-9793-49f5-aaa8-498c43e86e71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "In the Convolution layer below, W, dw, b, db all start as numpy arrays.\n",
    "They **should** be PhiTensors- if someone wants to print them, they should have to spend PB.\n",
    "\n",
    "However, when they're initialized in the Convolution layer, they have to be initialized as \n",
    "np arrays because the initial values are public information. But as soon as they are exposed to a DP Tensor, they should convert to a PhiTensor.\n",
    "\n",
    "However we should be able to pass the data to and from various layers to each other.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbcf3cc8-0c49-452f-9c05-ebffded0b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \"\"\"\n",
    "    If this is the first layer in a model, provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does NOT include the sample axis, N.),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_filter, filter_size, input_shape=None, stride=1):\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.out_shape = None\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.init = XavierInitialization()\n",
    "        self.activation = leaky_ReLU()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.input_shape is not None\n",
    "            input_shape = self.input_shape\n",
    "        else:\n",
    "            input_shape = prev_layer.out_shape\n",
    "\n",
    "        # input_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert len(input_shape) == 4\n",
    "\n",
    "        nb_batch, pre_nb_filter, pre_height, pre_width = input_shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        height = (pre_height - filter_height) // self.stride + 1\n",
    "        width = (pre_width - filter_width) // self.stride + 1\n",
    "\n",
    "        # output shape\n",
    "        self.out_shape = (nb_batch, self.nb_filter, height, width)\n",
    "        print(self.out_shape)\n",
    "\n",
    "        # filters\n",
    "        self.W = self.init((self.nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "        self.b = np.zeros((self.nb_filter,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # TODO: This could fail if the DP Tensor has < 4 dimensions\n",
    "        \n",
    "        # shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = input.shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "        # init\n",
    "        outputs = np.zeros((nb_batch, self.nb_filter, new_img_h, new_img_w))\n",
    "        \n",
    "        \n",
    "\n",
    "        # convolution operation\n",
    "        for x in np.arange(nb_batch):\n",
    "            for y in np.arange(self.nb_filter):\n",
    "                for h in np.arange(new_img_h):\n",
    "                    for w in np.arange(new_img_w):\n",
    "                        h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                        # patch: (input_depth, filter_h, filter_w)\n",
    "                        patch = input[x, :, h_shift: h_shift + filter_height, w_shift: w_shift + filter_width]\n",
    "                        outputs[x, y, h, w] = np.sum(patch.child * self.W[y]) + self.b[y]\n",
    "\n",
    "        # nonlinear activation\n",
    "        # self.last_output: (nb_batch, output_depth, image height, image width)\n",
    "        \n",
    "        # TODO: Min/max vals are direct function of private data- fix this when we have time\n",
    "        outputs = PhiTensor(\n",
    "            child=outputs,data_subjects=np.zeros_like(outputs), \n",
    "            min_vals=outputs.min(), max_vals=outputs.max()\n",
    "        )\n",
    "        self.last_output = self.activation.forward(outputs)\n",
    "\n",
    "        return self.last_output\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        assert pre_grad.shape == self.last_output.shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = self.last_input.shape\n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "        \n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "#         filter_h, filter_w = self.filter_size\n",
    "        old_img_h, old_img_w = self.last_input.shape[-2:]\n",
    "\n",
    "        # gradients\n",
    "        self.dW = np.zeros((self.W.shape))\n",
    "        self.db = np.zeros((self.b.shape))\n",
    "        delta = pre_grad * self.activation.derivative()\n",
    "\n",
    "        # dW\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            for t in np.arange(input_depth):\n",
    "                for h in np.arange(filter_height):\n",
    "                    for w in np.arange(filter_width):\n",
    "                        input_window = self.last_input[:, t,\n",
    "                                       h:old_img_h - filter_height + h + 1:self.stride,\n",
    "                                       w:old_img_w - filter_width + w + 1:self.stride]\n",
    "                        delta_window = delta[:, r]\n",
    "                        self.dW[r, t, h, w] = np.sum(input_window * delta_window) / nb_batch\n",
    "\n",
    "        # db\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            self.db[r] = np.sum(delta[:, r]) / nb_batch\n",
    "\n",
    "        # dX\n",
    "        if not self.first_layer:\n",
    "            layer_grads = np.zeros(self.last_input.shape)\n",
    "            for b in np.arange(nb_batch):\n",
    "                for r in np.arange(self.nb_filter):\n",
    "                    for t in np.arange(input_depth):\n",
    "                        for h in np.arange(new_img_h):\n",
    "                            for w in np.arange(new_img_w):\n",
    "                                h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                                layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width] += \\\n",
    "                                    self.W[r, t] * delta[b, r, h, w]\n",
    "            return layer_grads\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0bfa725-40be-4c48-9cd8-0c583a34c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Convolution(3, 3, input_shape=(1, 3, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6480805b-c623-4f5a-83b8-7bdc0dd3b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "c.connect_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "454d6256-d500-4669-a8f4-61d96c4d4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhiTensor(\n",
    "    child=np.random.rand(1, 3, 10, 10), \n",
    "    data_subjects=np.zeros((1,3,10,10)), min_vals=0, max_vals=1)\n",
    "\n",
    "res = c.forward(input=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df10cfa0-63cd-431b-bccd-d5c6b72516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, epsilon=1e-6, momentum=0.9, axis=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "\n",
    "        self.beta, self.dbeta = None, None\n",
    "        self.gamma, self.dgamma = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        n_in = prev_layer.out_shape[-1]\n",
    "        self.beta = np.zeros((n_in,))\n",
    "        self.gamma = np.ones((n_in,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        # N, D = x.shape\n",
    "        self.out_shape = input.shape\n",
    "\n",
    "        # step1: calculate the mean\n",
    "        xmu = input - input.mean(axis=0)\n",
    "        print(\"Subtracted mean\")\n",
    "        # step3:        \n",
    "        var = xmu.std(axis=0)\n",
    "        sqrtvar = (var + self.epsilon).sqrt()\n",
    "    \n",
    "        print(\"square rooted\")\n",
    "        ivar = PhiTensor(\n",
    "            child=sqrtvar.child ** -1,\n",
    "            data_subjects=sqrtvar.data_subjects,\n",
    "            min_vals=lra(data=1/sqrtvar.min_vals.data,shape=sqrtvar.shape),\n",
    "            max_vals=lra(data=1/sqrtvar.max_vals.data,shape=sqrtvar.shape)\n",
    "        )\n",
    "        \n",
    "        print(\"got past reciprocal\")\n",
    "\n",
    "\n",
    "        # step5: normalization->x^\n",
    "        xhat = xmu * ivar\n",
    "        print(\"got xhat\")\n",
    "\n",
    "        # step6: scale and shift\n",
    "        gammax = xhat * self.gamma\n",
    "        print(\"got gammax\")\n",
    "        out = gammax + self.beta\n",
    "        print(\"got out\")\n",
    "\n",
    "        self.cache = (xhat, xmu, ivar, sqrtvar, var)\n",
    "        print(\"cached\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        xhat, xmu, ivar, sqrtvar, var = self.cache\n",
    "\n",
    "        N, D = pre_grad.shape\n",
    "\n",
    "        # step6\n",
    "        self.dbeta = np.sum(pre_grad, axis=0)\n",
    "        dgammax = pre_grad\n",
    "        self.dgamma = np.sum(dgammax * xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "\n",
    "        # step5\n",
    "        divar = np.sum(dxhat * xmu, axis=0)\n",
    "        dxmu1 = dxhat * ivar \n",
    "\n",
    "        # step4\n",
    "        dsqrtvar = -1. / (sqrtvar ** 2) * divar\n",
    "        dvar = 0.5 * 1. / np.sqrt(var + self.epsilon) * dsqrtvar\n",
    "\n",
    "        # step3\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        dxmu2 = 2 * xmu * dsq  \n",
    "\n",
    "        # step2, \n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "\n",
    "        # step1, \n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "\n",
    "        # step0 done!\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self.gamma\n",
    "\n",
    "    @property\n",
    "    def grades(self):\n",
    "        return self.dbeta, self.dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cb34df6-5ccc-49a0-9377-a7036c8c4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (10, 3, 45, 45)\n",
    "\n",
    "smol_data = PhiTensor(\n",
    "    child=np.random.rand(*shape)*255, \n",
    "    data_subjects=np.zeros(shape), min_vals=0, max_vals=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae45546d-2e47-4470-a2fa-a6dd085c699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Convolution(3, 3, input_shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb8167-2a69-463a-8466-571388b439db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bn = BatchNorm()\n",
    "c.connect_to()\n",
    "\n",
    "c_out = c.forward(smol_data)\n",
    "bn.connect_to(c)\n",
    "\n",
    "bn.forward(c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88ac13b-56b4-4670-b006-ad4f9cc4599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.connect_to(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4fd82b6-d5a1-42ea-ab1c-b752be8cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = c.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbfa4e99-0dd2-4b0f-89f8-de4d02a31b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[ 1.40251878e+00  1.40759917e+00  1.38003531e+00  1.21287398e+00\n",
       "     9.97024823e-01  1.37393484e+00  1.04216294e+00  9.77529324e-01]\n",
       "   [ 1.23390671e+00  1.19049126e+00  9.04071564e-01  1.19749754e+00\n",
       "     1.07099766e+00  1.63699410e+00  1.43588909e+00  9.24735816e-01]\n",
       "   [ 1.30170905e+00  1.13319145e+00  1.52787230e+00  8.82345749e-01\n",
       "     1.13069262e+00  1.18042757e+00  1.02942697e+00  1.00675492e+00]\n",
       "   [ 1.20307709e+00  1.19424099e+00  1.12467341e+00  6.13610605e-01\n",
       "     1.20298431e+00  1.05406859e+00  1.04807631e+00  9.93963881e-01]\n",
       "   [ 9.75349670e-01  1.23965746e+00  1.00776759e+00  9.19905359e-01\n",
       "     9.21031859e-01  1.06005422e+00  1.00736797e+00  1.51906488e+00]\n",
       "   [ 1.67343777e+00  1.42212405e+00  1.14299389e+00  1.10227624e+00\n",
       "     1.33224262e+00  1.00696287e+00  9.63888150e-01  7.92041586e-01]\n",
       "   [ 1.06404727e+00  1.43051870e+00  2.67564543e-01  6.13392836e-01\n",
       "     1.04106005e+00  1.22431861e+00  9.12134147e-01  9.92752129e-01]\n",
       "   [ 1.60796194e+00  1.24725204e+00  1.33415039e+00  1.05173900e+00\n",
       "     1.26395689e+00  1.27331802e+00  7.45603831e-01  1.08813738e+00]]\n",
       "\n",
       "  [[ 1.79960531e-03 -3.38923038e-03 -2.55489094e-03  5.42175281e-02\n",
       "     1.20519442e-02  3.31807044e-02 -2.42891447e-03 -4.59224337e-03]\n",
       "   [-2.81764477e-03 -3.38087003e-04  3.00899992e-01 -6.18941691e-04\n",
       "     3.35203357e-01 -2.02242869e-03  8.19094970e-03 -1.24947198e-03]\n",
       "   [ 9.62499802e-04 -3.94910300e-03 -1.02815364e-03 -4.98741694e-03\n",
       "    -5.04345890e-03 -2.42914821e-03 -1.86391789e-03 -9.05084121e-04]\n",
       "   [-1.24257917e-03 -1.00025164e-03 -2.84159865e-03  9.83089368e-02\n",
       "     2.60766265e-01  3.45337512e-01 -2.11696183e-03 -4.04965385e-03]\n",
       "   [-1.69185751e-03 -5.38193329e-03 -3.82318933e-04  2.85236427e-02\n",
       "    -1.90695492e-03 -1.20617461e-03 -1.91890539e-03 -2.27672055e-03]\n",
       "   [-3.50897846e-03  3.34492517e-01 -4.92374872e-03  1.57230288e-02\n",
       "    -3.27164870e-03 -7.61069014e-03 -1.38363103e-03 -2.72695445e-03]\n",
       "   [-2.15004450e-03 -1.63207726e-03 -3.66329332e-04 -1.00268547e-03\n",
       "    -8.64563567e-04 -9.37406170e-04 -6.17936450e-03 -1.28175074e-03]\n",
       "   [-7.43911514e-04 -1.18880930e-03  1.89598680e-01 -2.53147883e-03\n",
       "     6.49955256e-02 -1.57465702e-03 -1.06141892e-03 -1.78593445e-03]]\n",
       "\n",
       "  [[-1.07636406e-03  3.08415124e-01 -1.30481631e-03  2.50767635e-01\n",
       "     1.21475965e-01 -3.63533980e-03 -1.87657889e-03 -1.03086173e-03]\n",
       "   [-1.07017057e-03 -3.52171274e-03  2.72993446e-01 -3.22303611e-03\n",
       "     8.49104319e-02 -1.18259171e-03  2.97926993e-01 -8.19869873e-04]\n",
       "   [-2.50247814e-03  1.38789803e-01  2.31990509e-01 -3.85049004e-04\n",
       "    -5.82824072e-04 -3.94111457e-03 -1.94016502e-03 -3.00014651e-03]\n",
       "   [ 4.83621715e-01 -1.97576299e-03 -1.15996666e-03  1.36130036e-01\n",
       "     9.36907863e-02  1.30480689e-02  4.94147979e-01 -6.40666170e-04]\n",
       "   [-1.47958874e-03  1.85520380e-01  5.94789367e-01 -1.02135937e-03\n",
       "    -7.67070809e-04 -6.47531131e-03  2.46089332e-01 -3.84031094e-03]\n",
       "   [-2.29459028e-03  2.97034795e-01 -4.99472124e-03 -1.30195389e-03\n",
       "     2.70423121e-01  1.92403494e-01  2.06688143e-01 -1.09722436e-03]\n",
       "   [ 2.75423310e-02 -4.00564298e-03  6.40763683e-01 -1.13925623e-03\n",
       "    -8.06729212e-04  3.45561692e-02  3.07526716e-01  3.74861995e-03]\n",
       "   [-3.78210572e-03  5.88855253e-01 -3.31135450e-03 -1.71919824e-03\n",
       "     1.07990009e-01 -4.42763012e-03  5.65270092e-01  2.88552655e-01]]]], min_vals=<lazyrepeatarray data: -0.007610690137191293 -> shape: (1, 3, 8, 8)>, max_vals=<lazyrepeatarray data: 0.016734377722378006 -> shape: (1, 3, 8, 8)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5f635-a953-429b-a2ba-d95165bb5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.forward(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16e99e-20fa-4702-bf4a-afed82b43525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(Layer):\n",
    "    \"\"\"Average pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.out_shape = 0\n",
    "        self.out_shape = None\n",
    "        self.input_shape = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert 5 > len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "\n",
    "        if np.ndim(input) == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = np.mean(input[a, b, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        elif np.ndim(input) == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = np.mean(input[a, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        length = np.prod(self.pool_size)\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "\n",
    "        if np.ndim(pre_grad) == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                            layer_grads[a, b, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                                pre_grad[a, b, h, w] / length\n",
    "\n",
    "        elif np.ndim(pre_grad) == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                        layer_grads[a, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                            pre_grad[a, h, w] / length\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads\n",
    "\n",
    "\n",
    "class MaxPool(Layer):\n",
    "    \"\"\"Max pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.out_shape = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        # prev_layer.out_shape: (nb_batch, ..., height, width)\n",
    "        assert len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        self.last_input = input\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "\n",
    "        if np.ndim(input) == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = np.max(input[a, b, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        elif np.ndim(input) == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = np.max(input[a, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "\n",
    "        if np.ndim(pre_grad) == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            patch = self.last_input[a, b, h:h + pool_h, w:w + pool_w]\n",
    "                            max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                            h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                            layer_grads[a, b, h_shift, w_shift] = pre_grad[a, b, a, w]\n",
    "\n",
    "        elif np.ndim(pre_grad) == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        patch = self.last_input[a, h:h + pool_h, w:w + pool_w]\n",
    "                        max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                        h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                        layer_grads[a, h_shift, w_shift] = pre_grad[a, a, w]\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be9b9e-95d0-4120-a1c5-45c096a74d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508b16d-6fa3-425d-9898-84171cd41258",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = AvgPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5506a6-4225-4d5d-b121-406d41b930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg.connect_to(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae462145-ca58-4a2a-a420-3b2e123f44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg.forward(bn.forward(c.forward(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b8c41-ef8b-421a-bc4e-52ea7f7ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp = MaxPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b4e77-d4de-4669-a8ae-69e33f273364",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp.connect_to(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599eb30-8347-46a8-91cd-458233ae47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp.forward(avg.forward(bn.forward(c.forward(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41a5cc-2098-438e-9e7c-d9978fadcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.out_shape = (None, n_out)\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.n_in is not None\n",
    "            n_in = self.n_in\n",
    "        else:\n",
    "            assert len(prev_layer.out_shape) == 2\n",
    "            n_in = prev_layer.out_shape[-1]\n",
    "\n",
    "        self.W = XavierInitialization((n_in, self.n_out))\n",
    "        self.b = np.zeros((self.n_out,))\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        self.last_input = input\n",
    "        return np.dot(input, self.W) + self.b\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        self.dW = np.dot(self.last_input.T, pre_grad)\n",
    "        self.db = np.mean(pre_grad, axis=0)\n",
    "        if not self.first_layer:\n",
    "            return np.dot(pre_grad, self.W.T)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d766d-997b-4360-bd26-77ca4b512447",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(n_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814059b-a806-4aa6-b8b9-6542bd98344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"Abstract optimizer base class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clip : float\n",
    "        If smaller than 0, do not apply parameter clip.\n",
    "    lr : float\n",
    "        The learning rate controlling the size of update steps\n",
    "    decay : float\n",
    "        Decay parameter for the moving average. Must lie in [0, 1) where\n",
    "        lower numbers means a shorter “memory”.\n",
    "    lr_min : float\n",
    "        When adapting step rates, do not move below this value. Default is 0.\n",
    "    lr_max : float\n",
    "        When adapting step rates, do not move above this value. Default is inf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.decay = decay\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iterations += 1\n",
    "\n",
    "        self.lr *= (1. / 1 + self.decay * self.iterations)\n",
    "        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17351b5-fd14-4dce-910a-a7591f7f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adamax(Optimizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta1 : float\n",
    "        Exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        Exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        Constant for numerical stability.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
    "           Adam: A Method for Stochastic Optimization.\n",
    "           arXiv preprint arXiv:1412.6980.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, *args, **kwargs):\n",
    "        super(Adamax, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.ms = None\n",
    "        self.vs = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        # init\n",
    "        self.iterations += 1\n",
    "        a_t = self.lr / (1 - np.power(self.beta1, self.iterations))\n",
    "        if self.ms is None:\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "        if self.vs is None:\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "\n",
    "        # update parameters\n",
    "        for i, (m, v, p, g) in enumerate(zip(self.ms, self.vs, params, grads)):\n",
    "            m = self.beta1 * m + (1 - self.beta1) * g\n",
    "            v = np.maximum(self.beta2 * v, np.abs(g))\n",
    "            p -= a_t * m / (v + self.epsilon)\n",
    "\n",
    "            self.ms[i] = m\n",
    "            self.vs[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e4df2-6b98-464f-ba2e-8602d7588178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction():\n",
    "    def forward(self, outputs, targets):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, outputs, targets):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735de6c7-43c7-49ec-aa2b-b40106a63452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "    def __init__(self, epsilon=1e-11):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        .. math:: L = -t \\\\log(p) - (1 - t) \\\\log(1 - p)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n",
    "        return np.mean(-np.sum(targets * np.log(outputs) + (1 - targets) * np.log(1 - outputs), axis=1))\n",
    "\n",
    "    def backward(self, outputs, targets):\n",
    "        \"\"\"Backward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = np.clip(outputs, self.epsilon, 1 - self.epsilon)\n",
    "        divisor = np.maximum(outputs * (1 - outputs), self.epsilon)\n",
    "        return (outputs - targets) / divisor\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0d5b1-0240-4871-91d1-aa0797b1595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "\n",
    "        self.loss = None\n",
    "        self.optimizer = Adamax\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer), \"PySyft doesn't recognize this kind of layer.\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss=BinaryCrossEntropy(), optimizer=Adamax()):\n",
    "        # check\n",
    "        # assert isinstance(self.layers[0], InputLayer)\n",
    "        self.layers[0].first_layer = True\n",
    "\n",
    "        # connect to\n",
    "        next_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer.connect_to(next_layer)\n",
    "            next_layer = layer\n",
    "        # for pre_layer, layer in zip(self.layers[:-1], self.layers[1:]):\n",
    "        #     layer.connect_to(pre_layer)\n",
    "\n",
    "        # get loss class\n",
    "        self.loss = BinaryCrossEntropy()\n",
    "\n",
    "        # get optimizer class\n",
    "        self.optimizer = Adamax()\n",
    "\n",
    "    def fit(self, X, Y, max_iter=100, batch_size=64, shuffle=True,\n",
    "            validation_split=0., validation_data=None):\n",
    "\n",
    "        # prepare data\n",
    "        train_X = X #.astype(get_dtype()) if np.issubdtype(np.float64, X.dtype) else X\n",
    "        train_Y = Y #.astype(get_dtype()) if np.issubdtype(np.float64, Y.dtype) else Y\n",
    "\n",
    "        if 1. > validation_split > 0.:\n",
    "            split = int(train_Y.shape[0] * validation_split)\n",
    "            valid_X, valid_Y = train_X[-split:], train_Y[-split:]\n",
    "            train_X, train_Y = train_X[:-split], train_Y[:-split]\n",
    "        elif validation_data is not None:\n",
    "            valid_X, valid_Y = validation_data\n",
    "        else:\n",
    "            valid_X, valid_Y = None, None\n",
    "\n",
    "        iter_idx = 0\n",
    "        while iter_idx < max_iter:\n",
    "            iter_idx += 1\n",
    "\n",
    "            # shuffle\n",
    "            if shuffle:\n",
    "                seed = np.random.randint(111, 1111111)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_X)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_Y)\n",
    "\n",
    "            # train\n",
    "            train_losses, train_predicts, train_targets = [], [], []\n",
    "            for b in range(train_Y.shape[0] // batch_size):\n",
    "                batch_begin = b * batch_size\n",
    "                batch_end = batch_begin + batch_size\n",
    "                x_batch = train_X[batch_begin:batch_end]\n",
    "                y_batch = train_Y[batch_begin:batch_end]\n",
    "\n",
    "                # forward propagation\n",
    "                y_pred = self.predict(x_batch)\n",
    "\n",
    "                # backward propagation\n",
    "                next_grad = self.loss.backward(y_pred, y_batch)\n",
    "                for layer in self.layers[::-1]:\n",
    "                    next_grad = layer.backward(next_grad)\n",
    "\n",
    "                # get parameter and gradients\n",
    "                params = []\n",
    "                grads = []\n",
    "                for layer in self.layers:\n",
    "                    params += layer.params\n",
    "                    grads += layer.grads\n",
    "\n",
    "                # update parameters\n",
    "                self.optimizer.update(params, grads)\n",
    "\n",
    "                # got loss and predict\n",
    "                train_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                train_predicts.extend(y_pred)\n",
    "                train_targets.extend(y_batch)\n",
    "\n",
    "            # output train status\n",
    "            runout = \"iter %d, train-[loss %.4f, acc %.4f]; \" % (\n",
    "                iter_idx, float(np.mean(train_losses)), float(self.accuracy(train_predicts, train_targets)))\n",
    "\n",
    "            # runout = \"iter %d, train-[loss %.4f, ]; \" % (\n",
    "            #     iter_idx, float(np.mean(train_losses)))\n",
    "\n",
    "            if valid_X is not None and valid_Y is not None:\n",
    "                # valid\n",
    "                valid_losses, valid_predicts, valid_targets = [], [], []\n",
    "                for b in range(valid_X.shape[0] // batch_size):\n",
    "                    batch_begin = b * batch_size\n",
    "                    batch_end = batch_begin + batch_size\n",
    "                    x_batch = valid_X[batch_begin:batch_end]\n",
    "                    y_batch = valid_Y[batch_begin:batch_end]\n",
    "\n",
    "                    # forward propagation\n",
    "                    y_pred = self.predict(x_batch)\n",
    "\n",
    "                    # got loss and predict\n",
    "                    valid_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                    valid_predicts.extend(y_pred)\n",
    "                    valid_targets.extend(y_batch)\n",
    "\n",
    "                # output valid status\n",
    "                runout += \"valid-[loss %.4f, acc %.4f]; \" % (\n",
    "                    float(np.mean(valid_losses)), float(self.accuracy(valid_predicts, valid_targets)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        x_next = X\n",
    "        for layer in self.layers[:]:\n",
    "            x_next = layer.forward(x_next)\n",
    "        y_pred = x_next\n",
    "        return y_pred\n",
    "\n",
    "    def accuracy(self, outputs, targets):\n",
    "        y_predicts = np.argmax(outputs, axis=1)\n",
    "        y_targets = np.argmax(targets, axis=1)\n",
    "        acc = y_predicts == y_targets\n",
    "        return np.mean(acc)\n",
    "\n",
    "        # acc = 0\n",
    "        # for i in range(y_targets.shape[0]):\n",
    "        #     if y_targets[i] == y_predicts[i]:\n",
    "        #         acc += 1\n",
    "        # return acc / y_targets.shape[0]\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c60091-098e-47a1-9185-d03183a29258",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f38f9-418e-4258-b3bf-1156a96cf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5895ac-edd6-48ea-89d7-cc683e84e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebd63e-d182-497c-9492-bbc992a9d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cadf8a-2839-4b27-b510-242cb4677caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174bbe7-c741-4342-ac90-d607e89c7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0d9e3-c1c2-40b7-a6f0-71994ab3e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249e523-b0e8-4027-a1b5-2ae823880786",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = mnist.data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3226f9e-b69d-4d4c-8b8f-85fea513a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = mnist.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f878c1-3af5-4eb5-b898-d054c75b7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "nb_data = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4f2ce-85e1-45ca-b243-9731d0e27433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = images.reshape((-1, 1, 28, 28)) / 255.0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01844e70-7580-46c4-bb47-8978285f0939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf6e48-ed61-4042-9a70-593e51888dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.permutation(X_train)[:nb_data]\n",
    "y_train = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8b5ae-dacf-465e-92d0-e8553f4e67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "y_train = np.random.permutation(y_train)[:nb_data]\n",
    "n_classes = np.unique(y_train).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af1ac7-a3ad-42e0-b9e3-4ea225cf65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, nb_classes=None):\n",
    "    classes = np.unique(labels)\n",
    "    if nb_classes is None:\n",
    "        nb_classes = classes.size\n",
    "    one_hot_labels = np.zeros((labels.shape[0], nb_classes))\n",
    "    for i, c in enumerate(classes):\n",
    "        one_hot_labels[labels == c, i] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915304b-2e5a-4c56-9640-40dd48a5dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def __init__(self, outdim=2):\n",
    "        self.outdim = outdim\n",
    "        if outdim < 1:\n",
    "            raise ValueError('Dim must be >0, was %i', outdim)\n",
    "\n",
    "        self.last_input_shape = None\n",
    "        self.out_shape = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert len(prev_layer.out_shape) > 2\n",
    "\n",
    "        to_flatten = np.prod(prev_layer.out_shape[self.outdim - 1:])\n",
    "        flattened_shape = prev_layer.out_shape[:self.outdim - 1] + (to_flatten,)\n",
    "\n",
    "        self.out_shape = flattened_shape\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        self.last_input_shape = input.shape\n",
    "\n",
    "        # to_flatten = np.prod(self.last_input_shape[self.outdim-1:])\n",
    "        # flattened_shape = input.shape[:self.outdim-1] + (to_flatten, )\n",
    "        flattened_shape = input.shape[:self.outdim - 1] + (-1,)\n",
    "        return np.reshape(input, flattened_shape)\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        return np.reshape(pre_grad, self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9a5b3-772a-4888-8c63-62032bdf7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"A fully connected layer implemented as the dot product of inputs and\n",
    "    weights. Generally used to implemenent nonlinearities for layer post activations.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_out : int\n",
    "        Desired size or shape of layer output\n",
    "    n_in : int, or None\n",
    "        The layer input size feeding into this layer\n",
    "    activation : str, or npdl.activatns.Activation\n",
    "        Defaults to ``Tanh``\n",
    "    init : str, or npdl.initializations.Initializer\n",
    "        Initializer object to use for initializing layer weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.out_shape = (None, n_out)\n",
    "        self.init = XavierInitialization()\n",
    "        self.act_layer = Softmax_func()\n",
    "\n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.n_in is not None\n",
    "            n_in = self.n_in\n",
    "        else:\n",
    "            assert len(prev_layer.out_shape) == 2\n",
    "            n_in = prev_layer.out_shape[-1]\n",
    "\n",
    "        self.W = self.init((n_in, self.n_out))\n",
    "        self.b = np.zeros((self.n_out,))\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        \"\"\" Apply the forward pass transformation to the input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : numpy.array\n",
    "            input data\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            output data\n",
    "        \"\"\"\n",
    "        self.last_input = input\n",
    "        linear_out = np.dot(input, self.W) + self.b\n",
    "        act_out = self.act_layer.forward(linear_out)\n",
    "        return act_out\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        \"\"\"Apply the backward pass transformation to the input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        pre_grad : numpy.array\n",
    "            deltas back propagated from the adjacent higher layer\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            deltas to propagate to the adjacent lower layer\n",
    "        \"\"\"\n",
    "        act_grad = pre_grad * self.act_layer.derivative()\n",
    "        self.dW = np.dot(self.last_input.T, act_grad)\n",
    "        self.db = np.mean(act_grad, axis=0)\n",
    "        if not self.first_layer:\n",
    "            return np.dot(act_grad, self.W.T)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "class Softmax_func():\n",
    "    \"\"\"Softmax activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_forward = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\":math:`\\\\varphi(\\\\mathbf{x})_j =\n",
    "        \\\\frac{e^{\\mathbf{x}_j}}{\\sum_{k=1}^K e^{\\mathbf{x}_k}}`\n",
    "        where :math:`K` is the total number of neurons in the layer. This\n",
    "        activation function gets applied row-wise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float32\n",
    "            The activation (the summed, weighted input of a neuron).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        float32 where the sum of the row is 1 and each single value is in [0, 1]\n",
    "            The output of the softmax function applied to the activation.\n",
    "        \"\"\"\n",
    "        assert np.ndim(input) == 2\n",
    "        self.last_forward = input\n",
    "        x = input - np.max(input, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        s = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return s\n",
    "\n",
    "    def derivative(self, input=None):\n",
    "        \"\"\"Backward propagation.\n",
    "        Returns\n",
    "        -------\n",
    "        float32 \n",
    "            The derivative of Softmax function.\n",
    "        \"\"\"\n",
    "        last_forward = input if input else self.last_forward\n",
    "        return np.ones(last_forward.shape)    \n",
    "\n",
    "\n",
    "class Softmax(Dense):\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        super(Softmax, self).__init__(n_out, n_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47193d4d-5f0d-40f3-869d-a5472e68be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model()\n",
    "cnn.add(Convolution(1, (3, 3), input_shape=(None, 1, 28, 28)))\n",
    "cnn.add(AvgPool((2, 2)))\n",
    "cnn.add(Convolution(2, (4, 4)))\n",
    "cnn.add(AvgPool((2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Softmax(n_out=n_classes))\n",
    "cnn.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72ae62-50c8-43b4-87d4-a6979ef98c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e745e0-2ebf-4ba7-b589-672c4c67b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "cnn.fit(X_train, one_hot(y_train), max_iter=max_iter, validation_split=0.1, batch_size=100)\n",
    "tf = time()\n",
    "\n",
    "print(tf - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fd5a8-68c0-4343-8863-86d92de18831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hagrid",
   "language": "python",
   "name": "hagrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
