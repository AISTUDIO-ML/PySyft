{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfb1f3f-4a02-4075-8150-fb2a3c299823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.fixed_precision_tensor import FixedPrecisionTensor as FPT\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d17f-fa56-4752-8907-f5a7d0fd938f",
   "metadata": {},
   "source": [
    "## Torch Dataset equivalent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a748ba2-8e67-45c9-b7b2-b415a20a983e",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data,transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50,50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282abb3-fbbb-494e-92f0-cbd94025e3f6",
   "metadata": {},
   "source": [
    "We have to implement:\n",
    "- padding -> DONE!\n",
    "- random horizontal flip -> DONE!\n",
    "- random vertical flip -> DONE!\n",
    "- random rotation -> Done but not confident about resultant LRA, or what to do with 0 values.\n",
    "- normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d774fc0d-3f87-4847-8ce9-8278f7f13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PhiTensor(child=np.random.random((5,5)),data_subjects=np.ones((5,5)), min_vals=0, max_vals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a2adb-f737-4320-94a2-77bd8ce329a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[25570 56399   730  8828 11760]\n",
       " [47269 58911 52585 14162 17480]\n",
       " [ 4998 36673 64755  5453 20284]\n",
       " [53661 25222 22404 18595 38348]\n",
       " [20318 40817  4667 41123 47390]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0339b54c-3e28-40e0-a672-e4e24725842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[58911 47269 58911 52585 14162 17480 14162]\n",
       " [56399 25570 56399   730  8828 11760  8828]\n",
       " [58911 47269 58911 52585 14162 17480 14162]\n",
       " [36673  4998 36673 64755  5453 20284  5453]\n",
       " [25222 53661 25222 22404 18595 38348 18595]\n",
       " [40817 20318 40817  4667 41123 47390 41123]\n",
       " [25222 53661 25222 22404 18595 38348 18595]]), min_vals=<lazyrepeatarray data: 0 -> shape: (6, 6)>, max_vals=<lazyrepeatarray data: 1 -> shape: (6, 6)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pad(width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00870aa9-37e8-408a-a5ed-b25cc50a55af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[11760  8828   730 56399 25570]\n",
       " [17480 14162 52585 58911 47269]\n",
       " [20284  5453 64755 36673  4998]\n",
       " [38348 18595 22404 25222 53661]\n",
       " [47390 41123  4667 40817 20318]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_horizontal_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d677ebe-88ee-4d24-9c75-db92ab7d23b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[25570 56399   730  8828 11760]\n",
       " [47269 58911 52585 14162 17480]\n",
       " [ 4998 36673 64755  5453 20284]\n",
       " [53661 25222 22404 18595 38348]\n",
       " [20318 40817  4667 41123 47390]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_vertical_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492273fe-2df1-427d-ac8f-de1186542f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[    0 56547  3748     0     0]\n",
       " [    0 58095 53694 14856 17141]\n",
       " [ 6030 35822 64755  5599 19643]\n",
       " [53719 24770 22320 18059     0]\n",
       " [    0     0  4689 43159     0]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_rotation(degrees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71a184a-26e1-4d01-a407-6ca19e14b0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6934446 , 0.45189331, 0.61488947, 0.94801581, 0.51159517,\n",
       "        0.11348231],\n",
       "       [0.32738637, 0.65398665, 0.61949498, 0.51183404, 0.78208044,\n",
       "        0.02906494],\n",
       "       [0.11666689, 0.55300441, 0.38670903, 0.58121557, 0.86982795,\n",
       "        0.83271507],\n",
       "       [0.97768839, 0.36474543, 0.24288966, 0.35806759, 0.21061981,\n",
       "        0.12508679],\n",
       "       [0.78576927, 0.12810912, 0.96287653, 0.21117806, 0.88156767,\n",
       "        0.42658132]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.random((5,6))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8cb53e-7d49-4edd-9a92-caeba89b4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import rotate\n",
    "rotated = rotate(d, angle=20, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277f7691-54d4-4f6d-b88e-6931c308a35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.72643831, 0.15990978,\n",
       "        0.        ],\n",
       "       [0.58965845, 0.57311074, 0.55385421, 0.64559415, 0.88817223,\n",
       "        0.        ],\n",
       "       [0.26462835, 0.67426301, 0.44401368, 0.56402397, 0.44484788,\n",
       "        0.19668674],\n",
       "       [0.        , 0.51388471, 0.1776453 , 0.3437646 , 0.40383747,\n",
       "        0.61286804],\n",
       "       [0.        , 0.74787336, 0.34035612, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b3352d-2276-4503-89be-94882cca8c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668c0c-9421-406e-8b99-278d9c604b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df76a4-6fef-4e21-a349-d4bd39098c9a",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To Do:\n",
    "- Implement the layers needed for a ConvNet for DP Tensors:\n",
    "    - Conv layer\n",
    "    - BatchNorm2D\n",
    "    - LeakyReLU\n",
    "    - AvgPool2d\n",
    "    - Linear\n",
    "- Do 1 forward pass with these layers\n",
    "- Do 1 backprop with these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a52955-8982-405f-a810-155c2fee4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(image, out_channels, kernel_size, padding=0, strides=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b4b05f-a6d0-4d79-81e0-73a0df7e1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6329c81d-baea-45c5-a1ec-768c46b99b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1475bb83-f3ce-488b-8ea5-30815ec1ddd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"/home/e/Downloads/Dataset/10253/0/10253_idx5_x1001_y1001_class0.png\")\n",
    "img = cv2.resize(img, (50, 50))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e24fcc4a-422e-41fc-b615-305cf264b674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 178)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Pad(64, padding_mode=\"reflect\")(transforms.ToPILImage()(img)).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402e52-3481-4afd-a155-67f3925b50a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
