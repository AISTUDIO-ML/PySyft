{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfb1f3f-4a02-4075-8150-fb2a3c299823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.fixed_precision_tensor import FixedPrecisionTensor as FPT\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d17f-fa56-4752-8907-f5a7d0fd938f",
   "metadata": {},
   "source": [
    "## Torch Dataset equivalent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a748ba2-8e67-45c9-b7b2-b415a20a983e",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data,transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50,50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282abb3-fbbb-494e-92f0-cbd94025e3f6",
   "metadata": {},
   "source": [
    "We have to implement:\n",
    "- padding -> DONE!\n",
    "- random horizontal flip -> DONE!\n",
    "- random vertical flip -> DONE!\n",
    "- random rotation -> Done but not confident about resultant LRA, or what to do with 0 values. -> Might try training the model without random rotations to see how testing accuracy suffers.\n",
    "- normalize -> DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d774fc0d-3f87-4847-8ce9-8278f7f13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PhiTensor(child=np.random.random((5,5)),data_subjects=np.ones((5,5)), min_vals=0, max_vals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a2adb-f737-4320-94a2-77bd8ce329a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0339b54c-3e28-40e0-a672-e4e24725842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[46383 48092 46383 10064 64947  6397 64947]\n",
       " [13466 32320 13466 31539 53025 47558 53025]\n",
       " [46383 48092 46383 10064 64947  6397 64947]\n",
       " [54672 23758 54672 39889 31523 35489 31523]\n",
       " [55424 64122 55424 64644 44428  5558 44428]\n",
       " [23557 58499 23557 14045 58880 64741 58880]\n",
       " [55424 64122 55424 64644 44428  5558 44428]]), min_vals=<lazyrepeatarray data: 0 -> shape: (6, 6)>, max_vals=<lazyrepeatarray data: 1 -> shape: (6, 6)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pad(width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00870aa9-37e8-408a-a5ed-b25cc50a55af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_horizontal_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d677ebe-88ee-4d24-9c75-db92ab7d23b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[58499 23557 14045 58880 64741]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [32320 13466 31539 53025 47558]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_vertical_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492273fe-2df1-427d-ac8f-de1186542f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6396]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_rotation(degrees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b96ab3-cbad-4dd0-8255-2b79a289ba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[  -896 -38604  -2458  40514  29580]\n",
       " [ 30648  27230 -45408  64358 -52742]\n",
       " [-18020  43808  14242  -2490   5442]\n",
       " [ 62708  45312  63752  23320 -54420]\n",
       " [ 51462 -18422 -37446  52224  63946]]), min_vals=<lazyrepeatarray data: -1.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.0 -> shape: ()>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.normalize(mean=0.5, std=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b52c8c4-5c9a-496c-8043-c98c46ee515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49316406, 0.20547485, 0.48124695, 0.80909729, 0.72567749],\n",
       "       [0.73382568, 0.70774841, 0.15356445, 0.99101257, 0.09761047],\n",
       "       [0.36251831, 0.83422852, 0.60865784, 0.48100281, 0.54151917],\n",
       "       [0.97842407, 0.84570312, 0.98638916, 0.67791748, 0.08480835],\n",
       "       [0.8926239 , 0.35945129, 0.21430969, 0.8984375 , 0.98786926]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.child.decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668c0c-9421-406e-8b99-278d9c604b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df76a4-6fef-4e21-a349-d4bd39098c9a",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To Do:\n",
    "- Implement the layers needed for a ConvNet for DP Tensors:\n",
    "    - Conv layer\n",
    "    - BatchNorm2D\n",
    "    - LeakyReLU\n",
    "    - AvgPool2d\n",
    "    - Linear\n",
    "- Do 1 forward pass with these layers\n",
    "- Do 1 backprop with these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3552c7c-4e61-453d-ac72-e03bb8519579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.tensor.nn.conv_layers import Conv2d\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45320db0-0e08-4e0b-949d-e7a483636c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 3)\n",
      "(50, 50, 3) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"/home/e/Downloads/Dataset/10253/0/10253_idx5_x1001_y1001_class0.png\")\n",
    "print(img.shape)\n",
    "img = cv2.resize(img, (50, 50))\n",
    "print(img.shape, type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7400b14b-0a5f-4ffa-93dd-d848a1c1af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tensor = PhiTensor(child=img, data_subjects=[0]  , min_vals=0, max_vals=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b248c6-3890-4cee-97d9-bba052501218",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Conv2d(dp_tensor, in_channels=3, out_channels=32, kernel_size=3, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3888e6d3-0385-4945-94a0-22fc613d50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.tensor.nn.batch_norm import BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4322be65-971f-4424-ae0f-2698221d8821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4a3dce-a94c-464d-8e4d-107171a18461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BatchNorm2d(3)(torch.Tensor(img.reshape(1, *img.shape[::-1]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd8ade0-ef72-4f20-9bfc-ea3903f9e781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 50, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt2 = dp_tensor.copy()\n",
    "dpt2.child = dp_tensor.child.reshape(1, *dp_tensor.shape[::-1])\n",
    "BatchNorm2d(dpt2, 3).child.decode().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab3e83c7-f012-4bda-af0b-d3882e0cbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3714, -0.8791,  0.9668,  ...,  0.7882, -0.1645, -1.5639],\n",
       "          [ 0.9073, -0.4027, -1.8914,  ...,  1.2943,  1.5623,  1.6814],\n",
       "          [ 1.5028,  1.6516,  1.6516,  ...,  1.2348,  0.5798,  1.4730],\n",
       "          ...,\n",
       "          [-1.5043,  0.8775, -0.1943,  ..., -0.1348, -1.5639,  0.4012],\n",
       "          [-0.0157, -1.4150,  0.8775,  ...,  1.0562,  0.4309, -0.7004],\n",
       "          [ 1.0562,  0.5500, -0.5813,  ..., -0.9088,  1.0859,  0.1630]],\n",
       "\n",
       "         [[-0.9313,  1.0406,  0.5924,  ..., -0.2143, -1.5886,  0.3534],\n",
       "          [-0.2740, -1.8276,  0.4729,  ...,  1.1003,  0.5625, -0.5429],\n",
       "          [ 1.1003,  0.7119, -0.3039,  ..., -0.5130,  1.1302,  0.3833],\n",
       "          ...,\n",
       "          [ 0.8912, -0.2143, -1.7978,  ..., -2.0368, -0.9014,  0.2040],\n",
       "          [-1.2002,  0.3235, -0.1246,  ..., -1.2002, -2.8733, -1.4691],\n",
       "          [-0.3338, -1.8276, -0.0948,  ...,  0.6820,  0.0546, -1.4990]],\n",
       "\n",
       "         [[ 0.8778,  0.6089, -0.4665,  ..., -1.8332, -0.6906, -0.3993],\n",
       "          [-1.8108,  0.3625, -0.5785,  ..., -0.3097, -1.8780, -0.2425],\n",
       "          [ 0.5193, -0.5113,  1.0122,  ...,  0.7882,  0.6313, -0.3993],\n",
       "          ...,\n",
       "          [ 0.2505, -1.0042,  0.9450,  ...,  0.9226,  0.7434, -0.1752],\n",
       "          [ 1.1018,  0.6986, -0.1528,  ..., -2.1693, -1.1387, -0.4889],\n",
       "          [-1.9677, -0.6906,  0.1384,  ...,  0.9226,  0.2280,  1.0122]]]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BatchNorm2d(3)(torch.Tensor(img.reshape(1, *img.shape[::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a2faf9-e299-4d75-bf68-8f7bf699a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.37138367, -0.8790741 ,  0.96684265, ...,  0.78820801,\n",
       "          -0.16452026, -1.56384277],\n",
       "         [ 0.9072876 , -0.4026947 , -1.89134216, ...,  1.29434204,\n",
       "           1.56230164,  1.68139648],\n",
       "         [ 1.50274658,  1.65161133,  1.65161133, ...,  1.23480225,\n",
       "           0.57978821,  1.47297668],\n",
       "         ...,\n",
       "         [-1.50430298,  0.8775177 , -0.19429016, ..., -0.13475037,\n",
       "          -1.56384277,  0.40115356],\n",
       "         [-0.01565552, -1.41497803,  0.8775177 , ...,  1.05615234,\n",
       "           0.43092346, -0.70042419],\n",
       "         [ 1.05615234,  0.55001831, -0.5813446 , ..., -0.90884399,\n",
       "           1.0859375 ,  0.16296387]],\n",
       "\n",
       "        [[-0.93130493,  1.04057312,  0.59240723, ..., -0.21426392,\n",
       "          -1.58860779,  0.35339355],\n",
       "         [-0.27401733, -1.82762146,  0.47290039, ...,  1.10032654,\n",
       "           0.56253052, -0.54290771],\n",
       "         [ 1.10032654,  0.71191406, -0.30389404, ..., -0.51303101,\n",
       "           1.13020325,  0.38327026],\n",
       "         ...,\n",
       "         [ 0.89117432, -0.21426392, -1.79774475, ..., -2.03677368,\n",
       "          -0.90142822,  0.20401001],\n",
       "         [-1.20021057,  0.32351685, -0.12463379, ..., -1.20021057,\n",
       "          -2.87332153, -1.46910095],\n",
       "         [-0.33377075, -1.82762146, -0.09475708, ...,  0.68203735,\n",
       "           0.05462646, -1.49897766]],\n",
       "\n",
       "        [[ 0.87779236,  0.6089325 , -0.46650696, ..., -1.83322144,\n",
       "          -0.69055176, -0.39929199],\n",
       "         [-1.81080627,  0.36247253, -0.57852173, ..., -0.30966187,\n",
       "          -1.87802124, -0.2424469 ],\n",
       "         [ 0.51930237, -0.51130676,  1.01222229, ...,  0.78816223,\n",
       "           0.6313324 , -0.39929199],\n",
       "         ...,\n",
       "         [ 0.2504425 , -1.00422668,  0.94500732, ...,  0.92259216,\n",
       "           0.74336243, -0.17523193],\n",
       "         [ 1.10183716,  0.69854736, -0.15283203, ..., -2.16929626,\n",
       "          -1.13865662, -0.48890686],\n",
       "         [-1.96765137, -0.69055176,  0.13842773, ...,  0.92259216,\n",
       "           0.2280426 ,  1.01222229]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BatchNorm2d(dpt2, 3).child.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6751012-f1f9-4be6-8816-2bac342fdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6932b6-06b7-4822-8c15-7b5092db5ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
