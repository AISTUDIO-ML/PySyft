{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfb1f3f-4a02-4075-8150-fb2a3c299823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.fixed_precision_tensor import FixedPrecisionTensor as FPT\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d17f-fa56-4752-8907-f5a7d0fd938f",
   "metadata": {},
   "source": [
    "## Torch Dataset equivalent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a748ba2-8e67-45c9-b7b2-b415a20a983e",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data,transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50,50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282abb3-fbbb-494e-92f0-cbd94025e3f6",
   "metadata": {},
   "source": [
    "We have to implement:\n",
    "- padding -> DONE!\n",
    "- random horizontal flip -> DONE!\n",
    "- random vertical flip -> DONE!\n",
    "- random rotation\n",
    "- normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d774fc0d-3f87-4847-8ce9-8278f7f13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PhiTensor(child=np.random.random((5,5)),data_subjects=np.ones((5,5)), min_vals=0, max_vals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0339b54c-3e28-40e0-a672-e4e24725842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[11025 52911 11025 11852 52907 40450 52907]\n",
       " [43084 13191 43084 48217 30642 26407 30642]\n",
       " [11025 52911 11025 11852 52907 40450 52907]\n",
       " [51629 37878 51629 61768 45954  8197 45954]\n",
       " [49061  3122 49061 49645 37790 23996 37790]\n",
       " [39779 22066 39779 34725  6354 41179  6354]\n",
       " [49061  3122 49061 49645 37790 23996 37790]]), min_vals=<lazyrepeatarray data: 0 -> shape: (6, 6)>, max_vals=<lazyrepeatarray data: 1 -> shape: (6, 6)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pad(width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595a2adb-f737-4320-94a2-77bd8ce329a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[13191 43084 48217 30642 26407]\n",
       " [52911 11025 11852 52907 40450]\n",
       " [37878 51629 61768 45954  8197]\n",
       " [ 3122 49061 49645 37790 23996]\n",
       " [22066 39779 34725  6354 41179]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00870aa9-37e8-408a-a5ed-b25cc50a55af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[26407 30642 48217 43084 13191]\n",
       " [40450 52907 11852 11025 52911]\n",
       " [ 8197 45954 61768 51629 37878]\n",
       " [23996 37790 49645 49061  3122]\n",
       " [41179  6354 34725 39779 22066]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_horizontal_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d677ebe-88ee-4d24-9c75-db92ab7d23b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[22066 39779 34725  6354 41179]\n",
       " [ 3122 49061 49645 37790 23996]\n",
       " [37878 51629 61768 45954  8197]\n",
       " [52911 11025 11852 52907 40450]\n",
       " [13191 43084 48217 30642 26407]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_vertical_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71a184a-26e1-4d01-a407-6ca19e14b0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23011031, 0.3644519 , 0.68004898, 0.15120429, 0.02616149,\n",
       "        0.05222286],\n",
       "       [0.51401488, 0.75666307, 0.55287758, 0.76785532, 0.73138011,\n",
       "        0.40606271],\n",
       "       [0.35806345, 0.36251549, 0.39874383, 0.07892666, 0.59880948,\n",
       "        0.59459851],\n",
       "       [0.56205918, 0.50725207, 0.34225295, 0.40214695, 0.60677908,\n",
       "        0.79016939],\n",
       "       [0.28875121, 0.59718988, 0.623744  , 0.92301468, 0.10991451,\n",
       "        0.0010024 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.random((5,6))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e8cb53e-7d49-4edd-9a92-caeba89b4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28875121, 0.59718988, 0.623744  , 0.92301468, 0.10991451,\n",
       "        0.0010024 ],\n",
       "       [0.56205918, 0.50725207, 0.34225295, 0.40214695, 0.60677908,\n",
       "        0.79016939],\n",
       "       [0.35806345, 0.36251549, 0.39874383, 0.07892666, 0.59880948,\n",
       "        0.59459851],\n",
       "       [0.51401488, 0.75666307, 0.55287758, 0.76785532, 0.73138011,\n",
       "        0.40606271],\n",
       "       [0.23011031, 0.3644519 , 0.68004898, 0.15120429, 0.02616149,\n",
       "        0.05222286]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.flipud(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668c0c-9421-406e-8b99-278d9c604b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df76a4-6fef-4e21-a349-d4bd39098c9a",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To Do:\n",
    "- Implement the layers needed for a ConvNet for DP Tensors:\n",
    "    - Conv layer\n",
    "    - BatchNorm2D\n",
    "    - LeakyReLU\n",
    "    - AvgPool2d\n",
    "    - Linear\n",
    "- Do 1 forward pass with these layers\n",
    "- Do 1 backprop with these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a52955-8982-405f-a810-155c2fee4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(image, out_channels, kernel_size, padding=0, strides=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b4b05f-a6d0-4d79-81e0-73a0df7e1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6329c81d-baea-45c5-a1ec-768c46b99b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1475bb83-f3ce-488b-8ea5-30815ec1ddd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"/home/e/Downloads/Dataset/10253/0/10253_idx5_x1001_y1001_class0.png\")\n",
    "img = cv2.resize(img, (50, 50))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e24fcc4a-422e-41fc-b615-305cf264b674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 178)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Pad(64, padding_mode=\"reflect\")(transforms.ToPILImage()(img)).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402e52-3481-4afd-a155-67f3925b50a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
