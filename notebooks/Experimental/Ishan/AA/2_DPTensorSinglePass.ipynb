{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfb1f3f-4a02-4075-8150-fb2a3c299823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.fixed_precision_tensor import FixedPrecisionTensor as FPT\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d17f-fa56-4752-8907-f5a7d0fd938f",
   "metadata": {},
   "source": [
    "## Torch Dataset equivalent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a748ba2-8e67-45c9-b7b2-b415a20a983e",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data,transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50,50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282abb3-fbbb-494e-92f0-cbd94025e3f6",
   "metadata": {},
   "source": [
    "We have to implement:\n",
    "- padding -> DONE!\n",
    "- random horizontal flip -> DONE!\n",
    "- random vertical flip -> DONE!\n",
    "- random rotation -> Done but not confident about resultant LRA, or what to do with 0 values.\n",
    "- normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d774fc0d-3f87-4847-8ce9-8278f7f13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PhiTensor(child=np.random.random((5,5)),data_subjects=np.ones((5,5)), min_vals=0, max_vals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a2adb-f737-4320-94a2-77bd8ce329a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0339b54c-3e28-40e0-a672-e4e24725842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[46383 48092 46383 10064 64947  6397 64947]\n",
       " [13466 32320 13466 31539 53025 47558 53025]\n",
       " [46383 48092 46383 10064 64947  6397 64947]\n",
       " [54672 23758 54672 39889 31523 35489 31523]\n",
       " [55424 64122 55424 64644 44428  5558 44428]\n",
       " [23557 58499 23557 14045 58880 64741 58880]\n",
       " [55424 64122 55424 64644 44428  5558 44428]]), min_vals=<lazyrepeatarray data: 0 -> shape: (6, 6)>, max_vals=<lazyrepeatarray data: 1 -> shape: (6, 6)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pad(width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00870aa9-37e8-408a-a5ed-b25cc50a55af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_horizontal_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d677ebe-88ee-4d24-9c75-db92ab7d23b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[58499 23557 14045 58880 64741]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [48092 46383 10064 64947  6397]\n",
       " [32320 13466 31539 53025 47558]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_vertical_flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492273fe-2df1-427d-ac8f-de1186542f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[32320 13466 31539 53025 47558]\n",
       " [48092 46383 10064 64947  6396]\n",
       " [23758 54672 39889 31523 35489]\n",
       " [64122 55424 64644 44428  5558]\n",
       " [58499 23557 14045 58880 64741]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_rotation(degrees=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b96ab3-cbad-4dd0-8255-2b79a289ba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[  -896 -38604  -2458  40514  29580]\n",
       " [ 30648  27230 -45408  64358 -52742]\n",
       " [-18020  43808  14242  -2490   5442]\n",
       " [ 62708  45312  63752  23320 -54420]\n",
       " [ 51462 -18422 -37446  52224  63946]]), min_vals=<lazyrepeatarray data: -1.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.0 -> shape: ()>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.normalize(mean=0.5, std=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b52c8c4-5c9a-496c-8043-c98c46ee515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49316406, 0.20547485, 0.48124695, 0.80909729, 0.72567749],\n",
       "       [0.73382568, 0.70774841, 0.15356445, 0.99101257, 0.09761047],\n",
       "       [0.36251831, 0.83422852, 0.60865784, 0.48100281, 0.54151917],\n",
       "       [0.97842407, 0.84570312, 0.98638916, 0.67791748, 0.08480835],\n",
       "       [0.8926239 , 0.35945129, 0.21430969, 0.8984375 , 0.98786926]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.child.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71a184a-26e1-4d01-a407-6ca19e14b0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97835989, 0.72904857, 0.51824965, 0.42437693, 0.75399854,\n",
       "        0.76029971],\n",
       "       [0.92208263, 0.84291823, 0.23542029, 0.59554622, 0.56608738,\n",
       "        0.82269911],\n",
       "       [0.41104667, 0.97923101, 0.12877033, 0.25893563, 0.76940868,\n",
       "        0.29018862],\n",
       "       [0.37586648, 0.05861313, 0.41366968, 0.42895079, 0.83388318,\n",
       "        0.69240145],\n",
       "       [0.53523728, 0.5853118 , 0.66683455, 0.33670758, 0.54811128,\n",
       "        0.90886165]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.random((5,6))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8cb53e-7d49-4edd-9a92-caeba89b4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import rotate\n",
    "rotated = rotate(d, angle=20, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277f7691-54d4-4f6d-b88e-6931c308a35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.72643831, 0.15990978,\n",
       "        0.        ],\n",
       "       [0.58965845, 0.57311074, 0.55385421, 0.64559415, 0.88817223,\n",
       "        0.        ],\n",
       "       [0.26462835, 0.67426301, 0.44401368, 0.56402397, 0.44484788,\n",
       "        0.19668674],\n",
       "       [0.        , 0.51388471, 0.1776453 , 0.3437646 , 0.40383747,\n",
       "        0.61286804],\n",
       "       [0.        , 0.74787336, 0.34035612, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b3352d-2276-4503-89be-94882cca8c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380f3777-8883-434e-b945-bae514a876a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f053715-bfa6-4068-8e46-d73eb4abb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668c0c-9421-406e-8b99-278d9c604b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df76a4-6fef-4e21-a349-d4bd39098c9a",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To Do:\n",
    "- Implement the layers needed for a ConvNet for DP Tensors:\n",
    "    - Conv layer\n",
    "    - BatchNorm2D\n",
    "    - LeakyReLU\n",
    "    - AvgPool2d\n",
    "    - Linear\n",
    "- Do 1 forward pass with these layers\n",
    "- Do 1 backprop with these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a52955-8982-405f-a810-155c2fee4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(image, out_channels, kernel_size, padding=0, strides=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769141da-8e5d-49e4-ab5b-0d4b5d5ede4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1475bb83-f3ce-488b-8ea5-30815ec1ddd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"/home/e/Downloads/Dataset/10253/0/10253_idx5_x1001_y1001_class0.png\")\n",
    "img = cv2.resize(img, (50, 50))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4825be69-d452-4b0c-9b2e-c7ae5252473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_norm = transforms.ToTensor()(transforms.Pad(64, padding_mode=\"reflect\")(transforms.ToPILImage()(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feecef24-0f9d-4b80-88f1-f7578023e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7206)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e6fbb4-5e2f-487e-b6a1-058af0a85a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1548)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_norm.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e24fcc4a-422e-41fc-b615-305cf264b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])(pre_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9402e52-3481-4afd-a155-67f3925b50a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4411)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fdf175-1281-476d-aa0a-adb62f80701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1696cc6-9431-4f21-b052-b5a10de4499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (pre_norm - pre_norm.mean()) / (pre_norm.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70cebac0-606d-4262-aad5-14bddac7859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0548e-08)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a0c8ea7-c225-4de3-b13d-0f83caa5accd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dfdc56c-7d1e-410f-a3eb-02491d8ea380",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.random((3, 5, 5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8010029-e2b0-43dd-a5c9-af9ee35f864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.02642461 0.56923667 0.22130778 0.31342289 0.11633154]\n",
      " [0.12364862 0.39458504 0.65386739 0.29517113 0.84490691]\n",
      " [0.70352203 0.29752624 0.11884652 0.83865271 0.64257607]\n",
      " [0.46191271 0.37322436 0.66233822 0.20258328 0.65426208]\n",
      " [0.0085115  0.73365111 0.01689818 0.3798477  0.56232386]]\n",
      "1 [[0.44206425 0.67285731 0.61315491 0.37023281 0.39998368]\n",
      " [0.26211304 0.39814871 0.69543346 0.46826847 0.8743573 ]\n",
      " [0.41289791 0.7571391  0.96612177 0.79737452 0.2898907 ]\n",
      " [0.43762697 0.91868411 0.04907501 0.20053439 0.47844655]\n",
      " [0.98855054 0.17506782 0.69149578 0.49048932 0.98230032]]\n",
      "2 [[0.23462324 0.10813466 0.40229739 0.49501227 0.54822249]\n",
      " [0.86502586 0.37189566 0.74206779 0.59223384 0.80024581]\n",
      " [0.23309564 0.32543315 0.75521786 0.42293359 0.69048595]\n",
      " [0.26698754 0.04232397 0.43985301 0.10540017 0.12507499]\n",
      " [0.20655178 0.51363049 0.60833977 0.15641146 0.16842811]]\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(d):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4178084a-f226-43f3-85ec-e985f8ec198d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,5,5) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38519/26781765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,5,5) (3,1) "
     ]
    }
   ],
   "source": [
    "d - np.ones((3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0e007-ebcb-488e-98ba-f36ef063d50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
