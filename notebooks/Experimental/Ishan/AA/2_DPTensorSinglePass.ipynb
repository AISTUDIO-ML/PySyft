{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfb1f3f-4a02-4075-8150-fb2a3c299823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.fixed_precision_tensor import FixedPrecisionTensor as FPT\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d17f-fa56-4752-8907-f5a7d0fd938f",
   "metadata": {},
   "source": [
    "## Torch Dataset equivalent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a748ba2-8e67-45c9-b7b2-b415a20a983e",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data,transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (50,50))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "        \n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.Pad(64, padding_mode='reflect'),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train)\n",
    "dataset_valid = MyDataset(df_data=val,transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282abb3-fbbb-494e-92f0-cbd94025e3f6",
   "metadata": {},
   "source": [
    "We have to implement:\n",
    "- padding -> DONE; needs LRA Implementation\n",
    "- random horizontal flip\n",
    "- random vertical flip\n",
    "- random rotation\n",
    "- normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d774fc0d-3f87-4847-8ce9-8278f7f13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PhiTensor(child=np.random.random((5,5)),data_subjects=np.ones((5,5)), min_vals=0, max_vals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30d53f-b109-43ef-a030-3a677010c6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb316bd-4c03-416c-9a4a-125d85890483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[55504 63718 55504 27632 52084 51937 52084]\n",
       " [33834 36499 33834 22439 30165 62863 30165]\n",
       " [55504 63718 55504 27632 52084 51937 52084]\n",
       " [53553 60535 53553 12064 17115  5578 17115]\n",
       " [29879  4088 29879 58644 34088 28847 34088]\n",
       " [21054 48476 21054 21786 11059 17184 11059]\n",
       " [29879  4088 29879 58644 34088 28847 34088]]), min_vals=<lazyrepeatarray data: 0 -> shape: (6, 6)>, max_vals=<lazyrepeatarray data: 1 -> shape: (6, 6)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pad(width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e16de45b-b3e3-4480-ac09-96310e93e58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87432296, 0.20823816, 0.00098661, 0.95585973, 0.61810754],\n",
       "       [0.00534942, 0.46297501, 0.14584004, 0.12418829, 0.34781126],\n",
       "       [0.65523659, 0.24482093, 0.9297652 , 0.620728  , 0.15386172],\n",
       "       [0.22302492, 0.57638697, 0.14881784, 0.77577571, 0.30402905],\n",
       "       [0.89354963, 0.49164349, 0.55097254, 0.92057209, 0.01183032]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.random((5,5))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98a57ad6-9840-447f-8626-0497b951fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46297501, 0.00534942, 0.46297501, 0.14584004, 0.12418829,\n",
       "        0.34781126, 0.12418829],\n",
       "       [0.20823816, 0.87432296, 0.20823816, 0.00098661, 0.95585973,\n",
       "        0.61810754, 0.95585973],\n",
       "       [0.46297501, 0.00534942, 0.46297501, 0.14584004, 0.12418829,\n",
       "        0.34781126, 0.12418829],\n",
       "       [0.24482093, 0.65523659, 0.24482093, 0.9297652 , 0.620728  ,\n",
       "        0.15386172, 0.620728  ],\n",
       "       [0.57638697, 0.22302492, 0.57638697, 0.14881784, 0.77577571,\n",
       "        0.30402905, 0.77577571],\n",
       "       [0.49164349, 0.89354963, 0.49164349, 0.55097254, 0.92057209,\n",
       "        0.01183032, 0.92057209],\n",
       "       [0.57638697, 0.22302492, 0.57638697, 0.14881784, 0.77577571,\n",
       "        0.30402905, 0.77577571]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pad(d, pad_width=1, mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f94159bc-3c8f-407c-893a-020356f51213",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate((d[0:2], d, d[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b19a487d-3d10-4870-bb58-82001b2eeddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87432296, 0.20823816, 0.00098661, 0.95585973, 0.61810754],\n",
       "       [0.00534942, 0.46297501, 0.14584004, 0.12418829, 0.34781126],\n",
       "       [0.87432296, 0.20823816, 0.00098661, 0.95585973, 0.61810754],\n",
       "       [0.00534942, 0.46297501, 0.14584004, 0.12418829, 0.34781126],\n",
       "       [0.65523659, 0.24482093, 0.9297652 , 0.620728  , 0.15386172],\n",
       "       [0.22302492, 0.57638697, 0.14881784, 0.77577571, 0.30402905],\n",
       "       [0.89354963, 0.49164349, 0.55097254, 0.92057209, 0.01183032],\n",
       "       [0.22302492, 0.57638697, 0.14881784, 0.77577571, 0.30402905],\n",
       "       [0.89354963, 0.49164349, 0.55097254, 0.92057209, 0.01183032]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff43bd46-c067-4f9f-aa75-7c9e346fbabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "397b2bbf-0e96-42d6-bdeb-3eb635debc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87432296, 0.20823816, 0.00098661, 0.95585973, 0.61810754],\n",
       "       [0.00534942, 0.46297501, 0.14584004, 0.12418829, 0.34781126]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "629cbd67-1a65-42ad-8af9-ee7f99515dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(dp_tensor: PhiTensor, width: float, padding_mode:str=\"reflect\") -> PhiTensor:\n",
    "    if padding_mode == \"reflect\":\n",
    "        output_data = np.pad(dp_tensor.child.decode(), pad_width=width, mode=padding_mode)\n",
    "        \n",
    "        original_indexed = dp_tensor.data_subjects.data_subjects_indexed.copy()\n",
    "        output_subjects = DataSubjectList(\n",
    "            one_hot_lookup=dp_tensor.data_subjects.one_hot_lookup,\n",
    "            data_subjects_indexed=np.pad(original_indexed, pad_width=width, mode=padding_mode)\n",
    "        )\n",
    "        \n",
    "        output_min_vals = dp_tensor.min_vals\n",
    "        output_max_vals = dp_tensor.max_vals\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return PhiTensor(\n",
    "        child=FPT(output_data),\n",
    "        data_subjects=output_subjects,\n",
    "        min_vals=output_min_vals,\n",
    "        max_vals=output_max_vals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a023adc2-72cc-4798-884b-0f8b2f9e10a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "062d14be-5ee7-4519-a9c6-718177f56c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[59501 19298 59501 57800 17703 41541 17703]\n",
       " [35346 11596 35346 39638 23254 23946 23254]\n",
       " [59501 19298 59501 57800 17703 41541 17703]\n",
       " [63908 54675 63908 12507 28145 20465 28145]\n",
       " [53854 50163 53854 39356 58727  4275 58727]\n",
       " [29121 54846 29121 45775 58021  5107 58021]\n",
       " [53854 50163 53854 39356 58727  4275 58727]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad(a, width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "208ef8fa-687c-4841-8982-2a6ce11150cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[11596 35346 39638 23254 23946]\n",
       " [19298 59501 57800 17703 41541]\n",
       " [54675 63908 12507 28145 20465]\n",
       " [50163 53854 39356 58727  4275]\n",
       " [54846 29121 45775 58021  5107]]), min_vals=<lazyrepeatarray data: 0 -> shape: (5, 5)>, max_vals=<lazyrepeatarray data: 1 -> shape: (5, 5)>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9c884fef-3374-471c-9e4d-b27c0bb1445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "laze = lra(data=1, shape=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2276bfd0-c5ca-4b4f-b5b9-04c70e6124c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laze.data.size == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668c0c-9421-406e-8b99-278d9c604b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df76a4-6fef-4e21-a349-d4bd39098c9a",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "\n",
    "To Do:\n",
    "- Implement the layers needed for a ConvNet for DP Tensors:\n",
    "    - Conv layer\n",
    "    - BatchNorm2D\n",
    "    - LeakyReLU\n",
    "    - AvgPool2d\n",
    "    - Linear\n",
    "- Do 1 forward pass with these layers\n",
    "- Do 1 backprop with these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a52955-8982-405f-a810-155c2fee4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(image, out_channels, kernel_size, padding=0, strides=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b4b05f-a6d0-4d79-81e0-73a0df7e1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6329c81d-baea-45c5-a1ec-768c46b99b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1475bb83-f3ce-488b-8ea5-30815ec1ddd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"/home/e/Downloads/Dataset/10253/0/10253_idx5_x1001_y1001_class0.png\")\n",
    "img = cv2.resize(img, (50, 50))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e24fcc4a-422e-41fc-b615-305cf264b674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 178)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Pad(64, padding_mode=\"reflect\")(transforms.ToPILImage()(img)).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402e52-3481-4afd-a155-67f3925b50a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
