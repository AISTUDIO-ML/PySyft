{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a0663-06ec-4b40-8acd-90287760544a",
   "metadata": {},
   "source": [
    "# Differential Privacy\n",
    "\n",
    "> \"You can't stop change any more than you can stop the sun from setting.\" ~ Shmi Skywalker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a37a71-cd1f-401a-a8fe-8d4da49e06b4",
   "metadata": {},
   "source": [
    "In this lesson we'll be learning about **Differential Privacy.** This is divided into two sections.\n",
    "\n",
    "Section 1 will focus on **Intuition**: \n",
    "* Intuition\n",
    "    * What is differential privacy (DP)?\n",
    "    * How does DP work?\n",
    "        * How does adding noise protect privacy?\n",
    "        * How much noise do we add?\n",
    "        * What is the tradeoff?\n",
    "    * What is the privacy budget?\n",
    "        * Connection between privacy budget and risk\n",
    "    * What is epsilon?\n",
    "    \n",
    "Section 2 will focus on DP in **our codebase.**\n",
    "* DP in PySyft\n",
    "    * How DP in PySyft is different than DP elsewhere\n",
    "        * Adversarial\n",
    "        * Individual\n",
    "        * Automatic\n",
    "    * Differential Privacy Tensors\n",
    "        * PhiTensors\n",
    "        * GammaTensors\n",
    "        * Helper Classes:\n",
    "            * LazyRepeatArrays\n",
    "            * DataSubjectArrays\n",
    "    * Ledgers and Privacy Budget Accounting\n",
    "    * Sigma and noise addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3f897-ad31-4c82-92b6-63d2e269b191",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ba4cd2-7d00-41d3-bcab-565fcbd12223",
   "metadata": {},
   "source": [
    "## Motivation \n",
    "\n",
    "In a previous lesson, we had discussed how difficult it is to protect people's privacy when working with or releasing data. We discussed the Netflix prize, where participants were de-anonymized with shocking accuracy. We mentioned problems caused by Data Linkages, and talked about the copy problem.\n",
    "\n",
    "Differential Privacy is one of the Privacy Enhancing Technologies (PETs) that we had discussed in a previous session. Like other PETs, it tries to solve some of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc857d5e-6bc8-4e3a-9b3e-848bfd54a9f0",
   "metadata": {},
   "source": [
    "We'll unpack this in more detail. But first, let's quickly standardize some terminology.\n",
    "\n",
    "<img src=\"imgs/ds_terminology.png\">\n",
    "\n",
    "This is the standard \"journey\" of data in data science:\n",
    "* Raw data is collected from lots of people (like you and I, called **data subjects**). \n",
    "* This raw data is collected and often cleaned/preprocessed by **data owners**,\n",
    "* The data owners then pass on these datasets to their data scientists, who then test their algorithms or workflows on the data to draw useful conclusions, or build products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdd78b-de87-4fd6-88ee-0f652e131a61",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ee453-dcc7-477d-8207-61c10a3986b5",
   "metadata": {},
   "source": [
    "## Section 1: Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556472e1-5be0-4528-93d8-48742b25e836",
   "metadata": {},
   "source": [
    "Put simply, differential privacy is a mathematical guarantee that the output of an algorithm is similar when data belonging to one person is removed.\n",
    "\n",
    "<img src=\"imgs/dp_definition.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9f50e-7f85-4c06-841e-a09cc0f2aa2e",
   "metadata": {},
   "source": [
    "Because the outputs are so similar, adding even the tiniest amount of noise can completely hide the effect of the person's data.\n",
    "\n",
    "<img src=\"imgs/dp_similarity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d85b0b-fdc8-41a8-9934-12234c0c9829",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "This means the Data Scientist still gets a reasonable and accurate answer (~25.7) but won't clue into the effect of your data.\n",
    "\n",
    "So in a nutshell, DP makes it so that the data scientist doesn't work with just the datasets- they work with datasets plus some noise that serves to protect the privacy of the data subjects.\n",
    "\n",
    "<img src=\"imgs/dp_ds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e688f-01e5-4ce0-a341-20876fea7dea",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Thus far, we've seen how differential privacy (DP) leads to the addition of a small amount of noise to protect the privacy of someone's data.\n",
    "\n",
    "A reasonable next question to ask is ***how much noise do we add?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefdbd0-a3ce-4c57-97cc-48e99469aba4",
   "metadata": {},
   "source": [
    "### How Much Noise To Add? (in English)\n",
    "\n",
    "\n",
    "Let's start with an easy, intuitive answer:\n",
    "\n",
    "<img src=\"imgs/dp_enuf2hide.png\">\n",
    "\n",
    "In the image above, we haven't added enough noise to properly protect the person's privacy. We need to add more!\n",
    "\n",
    "\n",
    "<img src=\"imgs/dp_not_enuf.png\">\n",
    "\n",
    "This time, we went way overboard with adding noise. The Data Scientist is going to suffer a serious loss of accuracy in their calculations. We need to add less!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90557045-a9a9-4833-bace-014344e50407",
   "metadata": {},
   "source": [
    "### How Much Noise to Add? (in Math)\n",
    "\n",
    "\n",
    "DP is a mathematical guarantee, and so there is a mathematical answer to this question.\n",
    "But before we can answer this question of how much noise to add, we'll need to understand a key insight: **datasets are distributions.**\n",
    "\n",
    "\n",
    "This might seem a bit strange to some of you. Let's take a simple example.\n",
    "\n",
    "Imagine you had a dataset consisting of the numbers [1, 2, 3, 4 and 5].\n",
    "An easy way you could convert this from a dataset into a distribution is if you iterated through every datapoint in the dataset, and asked what was the probability of this number being in this dataset.\n",
    "\n",
    "You'd then get a graph that looks a lot like this:\n",
    "\n",
    "<img src=\"imgs/dp_datasets_distr.png\">\n",
    "\n",
    "\n",
    "Voila! Using this simple scheme, we've converted our dataset into a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3ac32-ee2c-4b70-8801-bbf9382dbc76",
   "metadata": {},
   "source": [
    "Now, you might ask- what was the point of that? Why did we need to convert our dataset into a distribution?\n",
    "\n",
    "Well, it turns out- there are lots of ways to compare two distributions! Here's 16 of them, just for illustration:\n",
    "\n",
    "<img src=\"imgs/dp_distr_comp.png\">\n",
    "\n",
    "This means we now have a way to compare two datasets- by converting them to distributions, and then comparing them using any of the methods shown in the image above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c49e3d-6b02-468b-a298-996d6b85ffda",
   "metadata": {},
   "source": [
    "You might ask- but Ishan, **Why is it important to compare datasets?** What does this have to do with figuring out how much noise we want to add?\n",
    "\n",
    "Well, let's revisit the definition of Differential Privacy:\n",
    "\n",
    "Differential Privacy: The outcome of an algorithm is ***similar*** when a single person's data is removed from a dataset.\n",
    "\n",
    "**Intuition:** As soon as we know how similar the outcomes are, we immediately know how much noise is enough, and how much is too much. And it's easier to calculate this similarity by thinking about our datasets as if they were distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357300e-47c9-4ea7-a9a2-9037d96d630a",
   "metadata": {},
   "source": [
    "### Introducing $\\epsilon$, the privacy budget\n",
    "\n",
    "This \"similarity\" or difference is captured in a parameter called $\\epsilon$ (**epsilon**). It's also called the **privacy budget.**\n",
    "\n",
    "$\\epsilon$ has different formulas (depending on which method we used to compare our datasets/distributions), but it is always a measure of how much your data affects the outcome of the result:\n",
    "\n",
    "<img src=\"imgs/dp_intro_epsilon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850121a3-82a6-4e47-85d2-f3597197b4c0",
   "metadata": {},
   "source": [
    "$\\epsilon$, or the privacy budget, is probably the most important idea in differential privacy, so it's worth taking some time to emphasize what is exactly is.\n",
    "\n",
    "\n",
    "The privacy budget, $\\epsilon$, is a measure of:\n",
    "* How much your data stands out\n",
    "    * Thus, it's also a measure of *privacy risk*; how likely your data is going to be identified\n",
    "* How much your data affects the outcome of the query or algorithm\n",
    "* How much noise is needed to hide your data's influence.\n",
    "\n",
    "**Note:** \n",
    "There is a mathematical definition of epsilon (see resources below), but thinking about it in this way helps to build intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385da44-0397-4eea-9c13-efbd7b71d864",
   "metadata": {},
   "source": [
    "### Effects of $\\epsilon$\n",
    "\n",
    "**The higher the $\\epsilon$, the more your data affects the outcome of an algorithm.**\n",
    "\n",
    "<img src=\"imgs/dp_epsilon_eff1.png\">\n",
    "\n",
    "For example:\n",
    "* Imagine if we're trying to calculate the net worth of all the people living inside a single city, and we did this for every city in the US. Warren Buffet and Elon Musk would have gigantic values of $\\epsilon$, whereas someone who's homeless probably has a much lower $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fa7aa-b51d-4246-814c-982cbe2e4f4b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6b04f-1c7f-46ba-8a46-e84e84cfab1e",
   "metadata": {},
   "source": [
    "**The higher the $\\epsilon$, the more noise needs to be added to protect your privacy.**\n",
    "\n",
    "This should make intuitive sense:\n",
    "* As we discussed above, the higher the epsilon, the more your data affects the outcome of an algorithm.\n",
    "* The more your data affects the outcome of an algorithm, the more noise you need to add to mask the effects of your data.\n",
    "\n",
    "<img src=\"imgs/dp_epsilon_eff2.png\">\n",
    "\n",
    "\n",
    "In theory, noise is a random value that depends on $\\epsilon$. In practice, we obtain it from sampling from a distribution where $\\epsilon$ is a parameter. \n",
    "\n",
    "Often, it's a normal (Gaussian) distribution where $\\epsilon$ affects the standard deviation.\n",
    "So the bigger the $\\epsilon$, the wider the range of values the noise will be likely be sampled from [1]:\n",
    "\n",
    "<img src=\"imgs/dp_sigma.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33970d9-4013-4738-be12-c622d403a0d8",
   "metadata": {},
   "source": [
    "[1]: We know this because of the empirical (68-95-99.7) rule in statistics, which says that when you sample a normal distribution, approximately 68% of the values will be within one standard deviation of the mean, 95% will be within 2 standard deviations of the mean, and 99.7% will be within 3 deviations of the mean, and so forth. (This rule is also how the company Six Sigma got its name! They aim to get manufacturing processes error rates to a six sigma rate.)\n",
    "\n",
    "<img src=\"imgs/dp_empirical_rule.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b2597-9d9b-4a73-b730-69f9db35016f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a77243-ed47-47a0-acd1-250501b15caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final Intuition- Why does DP work?\n",
    "\n",
    "I'd like to leave you with a intuitive, gut feeling as to why differential privacy, this way of adding noise, actually works. \n",
    "\n",
    "The intuition is that when it comes to protecting privacy, your data \"hides\" in the background of other people's data. The more your data fits in with other people's data, the easier it is to blend in and hide in plain sight.\n",
    "\n",
    "Put it this way- it's much easier to spot the horse in the first image than the second one.\n",
    "\n",
    "<img src=\"imgs/dp_horse.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199df156-7b58-42bb-b69d-c96c62c62c45",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3026c3-c9ee-4318-8b47-9f8818a4eec8",
   "metadata": {},
   "source": [
    "Put it another way- the best way to hide a data point is to surround it with other, very similar data points! They'll be hard to tell apart, and it'll be like trying to look for a piece of **hay** in a very large, fluffy haystack:\n",
    "\n",
    "\n",
    "<img src=\"imgs/dp_haystack.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222515c-d5b6-457e-a41f-5b0c84a78fbc",
   "metadata": {},
   "source": [
    "By adding noise, we're doing something very similar- we're making it hard to discern one data point from another; the same way how dumping all your hay in a haystack makes it hard to single out individual pieces of hay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8aa79-506f-4809-9d83-cdf7c7c6a1c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23b58aaf-7204-4e49-b09f-52f912ea2049",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "* Regular Differential Privacy allows you to learn high level statistics and trends (how big is the haystack? Am I running out of hay? Is my hay on fire?) but not individual data (is hay #5 turning blue?)\n",
    "    * You can answer questions such as \"What causes cancer?\" and not \"Does Walter White have cancer?\"\n",
    "* Differential Privacy works by adding noise to the output of an algorithm to protect privacy.\n",
    "* $\\epsilon$, the privacy budget, is an indicator of how likely it is that someone's data stands out and can be identified.\n",
    "    * The higher the $\\epsilon$, the more likely that person's data stands out and will be identified.\n",
    "    * $\\epsilon$ can be calculated in many ways, but it's always an indicator of the risk of being identified and having your privacy violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecaadf0-a248-4ff6-a1f3-8bf1ae57c54b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fbd93-ccb2-496f-8ca4-a487d322ec60",
   "metadata": {},
   "source": [
    "## Section 2: DP in PySyft\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae690a3d-910d-4376-a846-d8fcd9345a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
